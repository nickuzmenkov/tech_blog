{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<ul> <li>$n$ is the number of features plus bias term</li> <li>$m$ is the number of instances</li> <li>$*^{(i)}$ are instance\u2019s indices</li> <li>$\\boldsymbol{x}$ is the instance\u2019s feature vector (with $x_0$ always equal to $1$)</li> <li>$\\boldsymbol{X}$ is the $(m \\times n)$ training matrix</li> <li>$\\boldsymbol{w}$ is the models\u2019 parameter (weight) vector, containing feature weights $w_i$</li> <li>$\\boldsymbol{\\theta}$ is the model\u2019s parameter vector, containing the bias (intercept) term $\\theta_0$ and $(n-1)$ feature weights $w_i$</li> <li>$\\boldsymbol{y}$ is the vector of $m$ target values $y^{(i)}$</li> <li>$|*|_2$ is $L^2$ norm</li> </ul>"},{"location":"#expected-value","title":"Expected Value","text":"<p>For a random variable with $n$ possible outcomes $x_i$ with associated probabilities $p_i$ the expected value $E[X]$ is given by</p> <p>$$ E[X] = \\sum_{i=1}^n{x_ip_i} $$ For a random variable with infinitely many outcomes and probability density function $f$ the expected value is given by</p> <p>$$ E[X] = \\int_{-\\infty}^{\\infty}{xf(x)\\ dx} $$</p>"},{"location":"#central-moment","title":"Central Moment","text":"<p>The Central Moment (or moment about the mean) of degree $k$ is given by</p> <p>$$ \\mu_k = E[(X-E[X])^k] = \\int_{-\\infty}^{\\infty}{(x-\\mu)^kf(x)\\ dx} $$</p> <p>where $E[X]$ is the [[#Expected Value|Expected Value]].</p> <p>Obviously, the zeroth central moment $\\mu_0$ is equal to $1$, the first central moment $\\mu_1$ is equal to $0$, and the second central moment is equal to variance $\\sigma^2$. </p>"},{"location":"#standardized-moment","title":"Standardized Moment","text":"<p>The Standardized Moment of degree $k$ is given by</p> <p>$$ \\tilde{\\mu}_k = \\frac{\\mu_k}{\\sigma^k}  $$</p>"},{"location":"#skewness","title":"Skewness","text":"<p>Skewness is the third [[#Standardized Moment|Standardized Moment]] and is given by</p> <p>$$ \\tilde{\\mu}_3 = \\frac{E[(X-E[X])^3]}{\\sigma^3} $$</p> <p>Zero skewness indicates perfectly symmetric distribution, while positive skewness indicates higher density of smaller values and vice versa.</p> <p>![[assets/img/Statistics/Notation/01.png|400]]</p>"},{"location":"#code-pandasseriesskew","title":"code <code>pandas.Series.skew</code>.","text":""},{"location":"#kurtosis","title":"Kurtosis","text":"<p>Kurtosis is the fourth [[#Standardized Moment|Standardized Moment]] and is given by</p> <p>$$ \\tilde{\\mu}_4 = \\frac{E[(X-E[X])^4]}{\\sigma^4} $$</p> <p>Kurtosis determines the plateau width of the distribution. The higher the kurtosis the narrower the plateau and vice versa.</p> <p>![[assets/img/Statistics/Notation/02.png|400]]</p>"},{"location":"#code-pandasserieskurt","title":"code <code>pandas.Series.kurt</code>.","text":""},{"location":"#cdf","title":"CDF","text":"<p>Cumulative Distribution Function</p>"},{"location":"#pmf","title":"PMF","text":"<p>Probability Mass Function</p>"},{"location":"#rvs","title":"RVs","text":"<p>Random Variables</p>"},{"location":"#z-score","title":"Z-Score","text":"<p>The process of standardizing is essentially subtracting the mean and dividing it by standard deviation. This value is also called a z-score.</p>"},{"location":"#rectangular-data","title":"Rectangular Data","text":"<p>Rectangular data is essentially a table of any relational database.</p>"},{"location":"#degrees-of-freedom","title":"Degrees of Freedom","text":"<p>In a nutshell, this is just the sample size (number of observations) minus number of explanatory variables.</p>"},{"location":"#frequency-table","title":"Frequency Table","text":"<p>Frequency table is essentially just the histogram represented as a table.</p>"},{"location":"#contingency-table","title":"Contingency Table","text":"<p>Contingency table is a table of frequency distribution of the variables typically with an extra row and column representing the totals. For #example:</p> Action Headline A Headline B Headline C Total Click 14 8 12 34 No click 986 992 988 2,966 Total 1,000 1,000 1,000 3,000"},{"location":"#norm","title":"Norm","text":"<p>Norms are functions mapping vectors to non-negative values. The $L^p$ norm is given by</p> <p>$$ |x|p=\\left(\\sum{|x_i|^p}\\right)^{1/p} $$</p> <p>for $p \\in \\mathbb{R}, p \\geq 1$.</p>"},{"location":"#manhattan-distance","title":"Manhattan Distance","text":"<p>Manhattan distance is given by</p> <p>$$ \\text{Distance}=\\sqrt{\\sum_{i=1}^n{(x_i-y_i)^2}} $$</p>"},{"location":"#euclidean-distance","title":"Euclidean Distance","text":"<p>Euclidean distance is given by</p> <p>$$ \\text{Distance}=\\sum_{i=1}^n|x_i-y_i|^2 $$</p>"},{"location":"#bias","title":"Bias","text":"<p>Bias is the part of generalization error due to wrong assumptions (e.g. data linearity).</p>"},{"location":"#variance","title":"Variance","text":"<p>Variance is the part of generalization error due to the model\u2019s excessive sensitivity to small variations in data (e.g. high-degree polynomials).</p>"},{"location":"#white-box-model","title":"White Box Model","text":"<p>White Box Model is easy to interpret and draw insights from given model\u2019s parameters.</p>"},{"location":"#black-box-model","title":"Black Box Model","text":"<p>Unlike [[#White Box Model]], Black Box Moel does not provide intuitive results.</p>"},{"location":"basics/classification/","title":"Classification","text":""},{"location":"basics/classification/#binary-classification","title":"Binary classification","text":""},{"location":"basics/classification/#empty","title":"empty","text":""},{"location":"basics/classification/#multiclass-classification","title":"Multiclass classification","text":"<p>[[Naive Bayes|Naive Bayes]], [[Linear Models#Gradient Descent|SGD]], and [[Random Forest|Random Forest]] classifiers have native support for multiclass classification tasks.</p> <p>On the contrary, [[Linear Models#Logistic Regression|Logistic Regression]] and [[Support Vector Machines#Classification|SVC]] do not support it. However, these models can  be scaled to handle multiclass classification tasks as well using either one-versus-the-rest (OvR) or one-versus-one (OvO) approach.</p> <p>\ud83d\udca1This is exactly what sklearn implicitly does for such models.</p>"},{"location":"basics/classification/#one-versus-the-rest-ovr","title":"One-Versus-the-Rest (OvR)","text":"<p>Train class detectors for each class (e.g. class A, not class A). For inference choose the highest probability among all models. Requires $N$ models to be fitted with the eintire dataset.</p>"},{"location":"basics/classification/#code-sklearnmulticlassonevsrestclassifier","title":"code  <code>sklearn.multiclass.OneVsRestClassifier</code>.","text":""},{"location":"basics/classification/#one-versus-one-ovo","title":"One-Versus-One (OvO)","text":"<p>Train models to distinguish between each pair of classes. For inference choose the class winning maximum number of duels. Requires $N(N-1)/2$ models to be fitted with $2/N$ fraction of the dataset.</p>"},{"location":"basics/classification/#code-sklearnmulticlassonevsoneclassifier","title":"code   <code>sklearn.multiclass.OneVsOneClassifier</code>.","text":""},{"location":"basics/classification/#multilabel-classification","title":"Multilabel classification","text":""},{"location":"basics/classification/#empty_1","title":"empty","text":""},{"location":"basics/classification/#multioutput-classification","title":"Multioutput classification","text":"<p>Multioutput classification is generalization of multilabel classification to more than one class for each label.</p>"},{"location":"basics/classification/#strategies-for-imbalanced-data","title":"Strategies for Imbalanced Data","text":"<p>When one of the classes in the dataset is rare (e.g. fraud, churn, cancer, etc.) there\u2019s a decent probability of observing similar records in the majority class just by chance. This is likely to disrupt the whole training process.</p> <p>In this case one should also concern about the correct choice of metrics, since monitoring wrong metrics (e.g. accuracy) can yield satisfying results even if the model only predicts the majority class all the time.</p> <p>\ud83d\udca1 Stratified KFold is handy for multiclass classification problems (especially for those with imbalanced data) hence it makes sure each class is represented like in the original distribution.</p>"},{"location":"basics/classification/#undersampling","title":"Undersampling","text":"<p>Undersampling the majority class is usually applied when there\u2019s a lot of data available.</p>"},{"location":"basics/classification/#oversampling","title":"Oversampling","text":"<p>Oversampling the minority class is applied when the opposite is true. Underrepresented class\u2019 records can be bootstrapped forming a larger sample.</p>"},{"location":"basics/classification/#weighted-samples","title":"Weighted Samples","text":"<p>Applying weights is an option in both cases. Weights are selected so that the sum of weights of both classes are equal (e.g. if $p$ is the fraction of the minority class, then either the minority records can be weighted as $1/p$ or the majority records can be weighted as $p$).</p>"},{"location":"basics/classification/#examples","title":"Examples","text":""},{"location":"basics/classification/#multiclass-classification-example","title":"Multiclass Classification #example","text":"<p>![[assets/img/Statistics/Classification/01.png|700]]</p>"},{"location":"basics/classification/#sourcecode-classification-codeiris-multiclass-classification","title":"sourcecode [[Classification Code#Iris Multiclass Classification]].","text":""},{"location":"basics/classification/#multiloutput-classification-example","title":"Multiloutput Classification #example","text":"<p>![[assets/img/Statistics/Classification/02.png|700]]</p>"},{"location":"basics/classification/#sourcecode-classification-codemnist-denoising","title":"sourcecode [[Classification Code#MNIST Denoising]]","text":""},{"location":"basics/classification/#accuracy","title":"Accuracy","text":"<p>Accuracy is simply a fraction of all correct predictions</p> <p>$$ \\text{Accuracy}=\\frac{\\text{TP}+\\text{TN}}{n} $$</p> <p>where $\\text{TP}$ is the total number of true positives and $\\text{TN}$ is the total number of true negatives.</p>"},{"location":"basics/classification/#code-sklearnmetricsaccuracy_scorey_true-y_pred","title":"code <code>sklearn.metrics.accuracy_score(y_true, y_pred)</code>.","text":""},{"location":"basics/classification/#confusion-matrix","title":"Confusion Matrix","text":"<p>Confusion matrix is a way to present results in a form of table with rows of true values and columns of predicted values, giving all possible true/false rates. For #example:</p> True\\Predicted A B A 13 2 B 3 12 <p>\ud83d\udca1 #warning Confusion matrix is not necessarily symmetrical and thus cannot be cut along diagonal like the correlation matrix.</p>"},{"location":"basics/classification/#code-sklearnmetricsconfusion_matrixy_true-y_pred","title":"code <code>sklearn.metrics.confusion_matrix(y_true, y_pred)</code>.","text":""},{"location":"basics/classification/#precision","title":"Precision","text":"<p>Precision is the accuracy of the positive predictions (i.e. the ratio of correctly predicted positives to all predicted positives):</p> <p>$$ \\text{Precision}=\\frac{\\text{TP}}{\\text{TP}+\\text{FP}} $$</p> <p>A trivial perfect-precision-model is that one predicting a single positive making sure it\u2019s correct (i.e. the one of the highest confidence score).</p> <p>Can be drawn from confusion matrix: true positives in the column divided by the sum of the column. For #example from the [[#Confusion Matrix|above Confusion Matrix]] </p> <p>$$ \\text{Precision}=12/(12+2)=0.86 $$</p> <p>(i.e. when model predicts class B it is correct 86% of the time).</p>"},{"location":"basics/classification/#code-sklearnmetricsprecision_scorey_true-y_pred","title":"code <code>sklearn.metrics.precision_score(y_true, y_pred)</code>.","text":""},{"location":"basics/classification/#recall","title":"Recall","text":"<p>Recall (or true positive rate) is the ratio of correctly predicted positives to actual positives:</p> <p>$$ \\text{Recall}=\\frac{\\text{TP}}{\\text{TP}+\\text{FN}} $$</p> <p>A trivial perfect-recall-model is that one always predicting positives.</p> <p>Can be drawn from confusion matrix: true positives in the row divided by the sum of the row. E.g. from the [[#Confusion Matrix|above Confusion Matrix]]</p> <p>$$ \\text{Recall}=12/(12+3)=0.80 $$</p> <p>(i.e. model detected 80% of class B instances).</p>"},{"location":"basics/classification/#code-sklearnmetricsrecall_scorey_true-y_pred","title":"code <code>sklearn.metrics.recall_score(y_true, y_pred)</code>.","text":""},{"location":"basics/classification/#specificity","title":"Specificity","text":"<p>Similarly to recall, specificity (or true negative rate) is the ratio of correctly predicted negatives to actual negatives:</p> <p>$$ \\text{Specificity}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}} $$</p> <p>There\u2019s no direct <code>sklearn</code> implementation for specificity. But the required values can be drawn from the [[#Confusion Matrix]]. </p>"},{"location":"basics/classification/#f1-score","title":"F1-Score","text":"<p>F1-score is a way to measure precision-recall trade-off as a harmonic mean between them:</p> <p>$$ F_1=\\frac{2}{\\text{Precision}^{-1} + \\text{Recall}^{-1}} = \\frac{\\text{TP}}{\\text{TP} + 0.5(\\text{FP} + \\text{FN})} $$</p> <p>F1-score is computed as the harmonic mean (and not the regular mean) because it gives much more weight to low values. It thus can be high only if both precision and recall are high.</p> <p>F1-score, however, does not take into account true negative rate.</p> <p>\ud83d\udca1 A more generic $F_\\beta$ score applies additional weights, valuing one of precision or recall more than the other.</p>"},{"location":"basics/classification/#code-sklearnmetricsf1_scorey_true-y_pred","title":"code <code>sklearn.metrics.f1_score(y_true, y_pred)</code>.","text":""},{"location":"basics/classification/#precisionrecall-trade-off","title":"Precision/Recall Trade-Off","text":"<p>Precision and recall are mirroring metrics. Think about this:</p> <p>![[assets/img/Statistics/Classification Metrics/01.png|600]]</p> <ul> <li>As we increase the threshold there are fewer positives of higher confidence (those are less likely to be false positives), hence higher precision.</li> <li>As we decrease the threshold, more positives are predicted, raising the chance to predict all actual positives (and false positives), hence higher recall.</li> </ul> <p>In most cases, either high precision or high recall is required most:</p> <ul> <li>Adult content filter (positive is safe): high precision is required, low recall is acceptable.</li> <li>Shoplifter identifier (positive is suspect): high recall is required, low precision is acceptable.</li> </ul> <p>There are 2 visual representations for precision/recall trade-off: precision-recall (PR) curve and receiver operating curve (ROC). Both can give insights and help to choose a proper threshold based on precision and recall relative importance.</p> <p>\ud83d\udca1 It's generally advisable to choose PR curve when the positive class is rare or when you care more about false positives. Otherwise it is suggested to use ROC.</p>"},{"location":"basics/classification/#pr-precision-recall-curve","title":"PR (Precision-Recall) Curve","text":"<p>There are two ways to plot the PR curve. One is precision and recall versus the probability (<code>predict_proba</code>) or decision function (<code>decision_function</code>) threshold:</p> <p>![[assets/img/Statistics/Classification Metrics/03.png|500]] The other way is to plot precision versus recall directly: ![[assets/img/Statistics/Classification Metrics/02.png|700]]</p>"},{"location":"basics/classification/#code-sklearnmetricsprecision_recall_curve","title":"code <code>sklearn.metrics.precision_recall_curve</code>.","text":""},{"location":"basics/classification/#sourcecode-classification-metrics-codepr-curve","title":"sourcecode [[Classification Metrics Code#PR Curve]].","text":""},{"location":"basics/classification/#roc-receiver-operating-characteristics-curve","title":"ROC (Receiver Operating Characteristics) Curve","text":"<p>ROC curve shows $1-\\text{Specificity}$ (also called false positive rate) versus recall (true positive rate):</p> <p>![[04.png|500]]</p> <p>The analytic way to compute ROC is like this:</p> <ol> <li>Sort records descending by predicted probability.</li> <li>Compute cumulative specificity and recall on sorted records.</li> </ol>"},{"location":"basics/classification/#code-sklearnmetricsroc_curve","title":"code <code>sklearn.metrics.roc_curve</code>.","text":""},{"location":"basics/classification/#sourcecode-classification-metrics-coderoc-curve","title":"sourcecode [[Classification Metrics Code#ROC Curve]].","text":""},{"location":"basics/classification/#roc-auc-area-underneath-the-curve","title":"ROC AUC (Area Underneath the Curve)","text":"<p>A quantitative metric associated with the ROC curve is the area underneath the curve (AUC or ROC AUC). AUC is equal to 1.0 for a perfect classifier and to 0.5 for a random classifier.</p> <p>\ud83d\udca1 Thus ROC is drawn for all possible thresholds, ROC AUC is threshold independent.</p>"},{"location":"basics/classification/#code-sklearnmetricsroc_auc_score","title":"code <code>sklearn.metrics.roc_auc_score</code>.","text":""},{"location":"basics/data/","title":"Data and Sampling","text":""},{"location":"basics/data/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>Means of samples drawn from a population will be normally distributed around the population mean, even if the population itself is not normally distributed. The sample standard deviation will be roughly the same as that of the population.</p>"},{"location":"basics/data/#standard-error","title":"Standard error","text":"<p>Standard error is standard deviation of the normal distribution of sample means:</p> <p>$$ \\text{SE} = \\frac{\\sigma}{\\sqrt{n}} $$</p>"},{"location":"basics/data/#standard-error-on-difference","title":"Standard error on difference","text":"<p>Standard error on differrence in means:</p> <p>$$ \\text{SE (diff)}=\\sqrt{\\frac{\\sigma_x^2}{n_x} + \\frac{\\sigma_y^2}{n_y}} $$</p> <p>It is greater than one for each sample but lower than their sum. This corresponds to larger uncertainty when drawing several samples from the population.</p>"},{"location":"basics/data/#commong-sampling-bias-types","title":"Commong Sampling Bias Types","text":""},{"location":"basics/data/#selection-bias","title":"Selection bias","text":"<p>It\u2019s usually hard to draw a proper random sample from a population. Assessment of non-random sample characteristics can lead to dramatically incorrect conclusions.</p> <p>\ud83d\udca1 #story The Literary Digest magazine surveyed more than 10 million of its subscribers, as well as car and phone owners (those contacts were publicly available) all over the country to predict the winner of the presidential election in 1936. Results had proposed Landon\u2019s win with 57% votes. Instead, Roosevelt won with 60% votes. The poll was biased towards wealthier people (those who have magazine subscriptions, cars, and phones), who hence were more likely to vote Republican.</p>"},{"location":"basics/data/#publication-bias","title":"Publication bias","text":"<p>Assuming there are a lot of repeated studies all over the world each having multiple predictor variables vast search effect, some may find statistically significant patterns just by chance. Those reporting positive findings and/or correlations are more likely to be published.</p>"},{"location":"basics/data/#recall-bias","title":"Recall bias","text":"<p>People tend to create false memories based on their thoughts in present. This is why longitudinal studies are preferred to do cross-sectional studies.</p>"},{"location":"basics/data/#survivorship-bias","title":"Survivorship bias","text":"<p>For instance, average class test scores typically rise from freshman to senior year, as the least scoring students are being consistently kicked off.</p>"},{"location":"basics/data/#healthy-user-bias","title":"Healthy user bias","text":"<p>For instance, people who take vitamins regularly are healthier at least because they tend to care about themselves.</p>"},{"location":"basics/data/#bootstrap","title":"Bootstrap","text":"<p>Bootstrap is a process of replicating the original sample to form a hypothetical population to estimate standard errors and confidence intervals for the sample\u2019s characteristics.</p> <p>The same can be done by sampling with replacement (#code <code>sklearn.resample</code>) instead of replicating.</p> <p>Confidence intervals can be assessed numerically by drawing the characteristic of interest from $R$ random resamples of the original sample and getting level $p$ and $(1-p)$ quantiles. </p> <p>Bootstrap is also widely used for training model ensembles, e.g. each model is trained with bootstrap samples, then predictions averaged for inference, producing bagging (bootstrap aggregating) or pasting ensemble.</p> <p>\ud83d\udca1 #warning Bootstrap is not used to compensate for a small sample size, create new data, or impute the existing data. It merely gives the idea about samples' statistics drawn from population like the original sample</p>"},{"location":"basics/data/#common-distributions","title":"Common Distributions","text":""},{"location":"basics/data/#normal-distribution","title":"Normal Distribution","text":"<p>\ud83d\udca1 Assess how the sample is close to the normal distribution via #code <code>scipy.stats.probplot</code>.</p>"},{"location":"basics/data/#students-t-distribution","title":"Student\u2019s t-distribution","text":"<p>A family of distributions resembling the normal distribution, but with thicker tails. It\u2019s widely used for sample means, regression parameters, and the like.</p> <p>$$ \\overline{x} \\pm t_{n-1}(p) \\cdot \\frac{s}{\\sqrt{n}} $$</p>"},{"location":"basics/data/#binomial-distribution","title":"Binomial distribution","text":"<p>The frequency distribution of the number of successes ($x$) in a given number of trials ($n$) with specified probability of success in each trial ($p$).</p> <p>Distribution function</p> <p>$$ f(x, n, p) =  \\begin{pmatrix}     n \\     x \\ \\end{pmatrix} p^x (1 - p)^{n-x} = \\frac{n!}{x!(n-x)!}p^x (1 - p)^{n-x} $$</p> <p>Mean</p> <p>$$ \\overline{x}=n \\cdot p $$</p> <p>Variance</p> <p>$$ \\sigma^2=n\\cdot p \\cdot (1-p) $$</p> <p>\ud83d\udca1 For a large number of trials and $p$ close to 0.5 binomial distribution can be approximated by the normal distribution.</p>"},{"location":"basics/data/#code","title":"code","text":"<ul> <li>Probability of exactly $x$ successes in $n$ trials is given by <code>scipy.stats.binom.pmf(x=x, n=n, p=0.5)</code></li> <li>Probability of $x$ or fewer successes in $n$ trials <code>scipy.stats.binom.cdf(x=x, n=n, p=0.5)</code></li> </ul>"},{"location":"basics/data/#chi-squared-distribution","title":"Chi-squared distribution","text":""},{"location":"basics/data/#empty","title":"empty","text":""},{"location":"basics/data/#f-distribution","title":"F-distribution","text":""},{"location":"basics/data/#empty_1","title":"empty","text":""},{"location":"basics/data/#poisson-distribution","title":"Poisson Distribution","text":"<p>The frequency distribution of the number of events in sampled units of time or space.</p> <p>Distribution function</p> <p>$$ f(k)=\\exp{(-\\mu)} \\frac{\\mu^k}{k!} $$</p> <p>Mean</p> <p>$$ \\overline{x} = \\mu $$</p> <p>Variance</p> <p>$$ \\sigma^2 = \\mu $$</p>"},{"location":"basics/data/#code-scipystatspoissonrvsmu2-size100","title":"code <code>scipy.stats.poisson.rvs(mu=2, size=100)</code>.","text":""},{"location":"basics/data/#exponential-distribution","title":"Exponential Distribution","text":"<p>The frequency distribution of the time or distance from one event to the next event.</p> <p>Distribution function</p> <p>$$ f(x)=\\exp{(-x)}=\\lambda\\cdot \\exp{(-\\lambda \\cdot x)} $$</p> <p>\ud83d\udca1 In Poisson and exponential distributions, lambda must stay the same, which is usually not the case in real life. However, if the period over which lambda changes is much longer than the typical interval between events, one can divide time or space into chunks where lambda is roughly constant. When this is not the case, one must switch to Weibull distribution.</p>"},{"location":"basics/data/#code-scipystatsexponrvsscale02-size100","title":"code <code>scipy.stats.expon.rvs(scale=0.2, size=100)</code>.","text":""},{"location":"basics/data/#weibull-distribution","title":"Weibull Distribution","text":"<p>A generalized version of exponential distribution allows event rates to change over time.</p>"},{"location":"basics/data/#code-scipystatsweibull_minrvsc15-scale5000-size100","title":"code <code>scipy.stats.weibull_min.rvs(c=1.5, scale=5000, size=100)</code>.","text":""},{"location":"basics/eda/","title":"Exploratory Data Analysis","text":"<p>\ud83d\udca1 #story John W. Tukey first introduced EDA as a mandatory step before statistical inference in his paper \u201cThe future of Data Analysis\u201d in 1962.</p>"},{"location":"basics/eda/#code-for-advanced-plots","title":"code for advanced plots","text":"<ul> <li>Hexagonal binning <code>pandas.DataFrame.plot.hexbin</code></li> <li>Contour plot <code>seaborn.kdeplot</code></li> <li>Violin plot <code>seaborn.violinplot</code></li> </ul>"},{"location":"basics/eda/#data-types","title":"Data Types","text":"<ul> <li>Numeric<ul> <li>Continuous</li> <li>Discrete</li> </ul> </li> <li>Categorical<ul> <li>Binary (literally true/false)</li> <li>Ordinal (e.g. sizes: S, M, L, XL, etc.)</li> </ul> </li> </ul>"},{"location":"basics/eda/#central-tendency-estimation","title":"Central Tendency Estimation","text":""},{"location":"basics/eda/#mean","title":"Mean","text":""},{"location":"basics/eda/#weighted-mean","title":"Weighted mean","text":"<p>Weighted mean is given by</p> <p>$$ \\overline{x}w=\\frac{\\sum^n w_i x_i}{\\sum_{i=1}^n w_i} $$</p>"},{"location":"basics/eda/#trimmed-mean","title":"Trimmed mean","text":"<p>Trimmed mean is mean of a population with $p$ largest and $p$ smallest values omitted</p> <p>$$ \\overline{x}=\\frac{\\sum_{i=p+1}^{n-p}x_{(i)}}{n - 2p} $$</p>"},{"location":"basics/eda/#median","title":"Median","text":"<p>Median is given by</p>"},{"location":"basics/eda/#empty","title":"empty","text":"<p>$$ $$</p>"},{"location":"basics/eda/#weighted-median","title":"Weighted median","text":"<p>Weighted median is given by</p>"},{"location":"basics/eda/#empty_1","title":"empty","text":"<p>$$ $$</p>"},{"location":"basics/eda/#mode","title":"Mode","text":"<p>Mode is simply most frequent value. Only applicable for categorical data.</p>"},{"location":"basics/eda/#variability-metrics","title":"Variability Metrics","text":""},{"location":"basics/eda/#mean-absolute-deviation","title":"Mean Absolute Deviation","text":"<p>$$ \\text{Mean absolute deviation}=\\frac{\\sum_{i=1}^n |x_i-\\overline{x}|}{n} $$</p>"},{"location":"basics/eda/#variance","title":"Variance","text":"<p>Variance is given by</p> <p>$$ \\sigma^2=\\frac{\\sum_{i=1}^n (x_i -\\overline{x})^2}{n-1} $$</p>"},{"location":"basics/eda/#standard-deviation","title":"Standard Deviation","text":"<p>Standard deviation is given by</p> <p>$$ \\sigma=\\sqrt{\\frac{\\sum_{i=1}^n (x-\\overline{x})^2}{n-1}} $$</p>"},{"location":"basics/eda/#median-absolute-deviation-from-the-median","title":"Median Absolute Deviation from the Median","text":"<p>Median Absolute Deviation from the Median is given by</p>"},{"location":"basics/eda/#empty_2","title":"empty","text":"<p>$$ $$</p> <p>\ud83d\udca1 The median absolute deviation from the median is prone to outliers (like the [[#Median|Median]] itself), unlike [[#Mean Absolute Deviation|Mean Absolute Deviation]], [[#Variance|Variance]], and [[#Standard Deviation|Standard Deviation]].</p>"},{"location":"basics/eda/#interquartile-range","title":"Interquartile Range","text":"<p>Interquartile Range (IQR) is a difference between the 25th and the 75th percentiles.</p>"},{"location":"basics/eda/#exploring-data-distribution","title":"Exploring Data Distribution","text":""},{"location":"basics/eda/#histogram","title":"Histogram","text":"<p>Histograms and percentiles are used for the same purpose, roughly speaking. The core distinction is that histograms have an uneven number of samples but even bin width and vice versa.</p>"},{"location":"basics/eda/#density-plot","title":"Density Plot","text":"<p>A density plot can be treated as either a smoothed version of a histogram or an approximation of the distribution function.</p>"},{"location":"basics/eda/#code-pandasdataframeplotdensity","title":"code <code>pandas.DataFrame.plot.density</code>.","text":""},{"location":"basics/eda/#correlation","title":"Correlation","text":"<p>Most widespread correlation metric is the Pearson\u2019s correlation coefficient:</p> <p>$$ r=\\frac{\\sum_{i=1}^n(x-\\overline{x})(y-\\overline{y})}{(n-1)s_xs_y} $$</p> <p>It\u2019s highly prone to outliers and uncovers only linear correlations.</p>"},{"location":"basics/eda/#examples","title":"Examples","text":""},{"location":"basics/eda/#correlation-matrix-example","title":"Correlation Matrix #example","text":"<p>![[assets/img/Statistics/Exploratory Data Analysis/01.png|500]]</p>"},{"location":"basics/eda/#sourcecode-exploratory-data-analysis-codecorrelation-matrix","title":"sourcecode [[Exploratory Data Analysis Code#Correlation Matrix]].","text":""},{"location":"basics/ensemble/","title":"Ensemble","text":""},{"location":"basics/ensemble/#voting-classifiers","title":"Voting Classifiers","text":"<p>It can be shown from the [[Data and Sampling#Binomial distribution|Binomial distribution]] that an ensemble of weak learners (i.e. only slightly better than random gessing) can yield overwhelmingly high accuracy in combination.</p> <p>\ud83d\udca1 #warning This is only the case when all the models are independent, which is clearly not the case with models trained on the same data. On the contrary, they tend to make the same mistakes which can further reduce the model.</p> <p>So the major issue here is to get models as diverse as possible to avoid correlated errors.</p>"},{"location":"basics/ensemble/#hard-voting-classifier","title":"Hard Voting Classifier","text":"<p>Hard voting classifier simply takes the [[Exploratory Data Analysis#Mode|mode]] of individual models.</p>"},{"location":"basics/ensemble/#soft-voting-classifier","title":"Soft Voting Classifier","text":"<p>Soft voting classifier averages probability of each class and takes the highest probability, thus giving more weight to very confident votes.</p> <p>\ud83d\udca1 #warning In general, soft voting classifiers are better, but they require all models to be able to predict probabilities.</p>"},{"location":"basics/ensemble/#code","title":"code","text":"<ul> <li><code>sklearn.ensemble.VotingClassifier</code></li> <li><code>sklearn.ensemble.VotingRegressor</code></li> </ul>"},{"location":"basics/ensemble/#bagging-and-pasting","title":"Bagging and Pasting","text":"<p>One way to achieve model diversity is to train each predictor on different random subsets of the training data.</p> <p>This generally leads to bot higher [[base/Statistics/Notation#Bias|bias]] and [[base/Statistics/Notation#Variance|variance]] for each predictor, but the ensemble often has roughly the same bias but lower variance than a single predictor trained on the entire original training set.</p>"},{"location":"basics/ensemble/#bagging","title":"Bagging","text":"<p>Bagging is short for [[Data and Sampling#Bootstrap|Bootstrap]] Aggregating. It denotes forming random subsets from initial dataset with replacement (bootstrapping the data) and training each predictor on random subsamples.</p> <p>\ud83d\udca1Drawing samples with replacement means that the same instance can occur multiple times not only across multiple predictors' dataset, but also across the same dataset.</p>"},{"location":"basics/ensemble/#code_1","title":"code","text":"<ul> <li><code>sklearn.ensemble.BaggingClassifier(..., bootstrap=True)</code>.</li> <li><code>sklearn.ensemble.BaggingRegressor(..., bootstrap=True)</code>.</li> </ul>"},{"location":"basics/ensemble/#pasting","title":"Pasting","text":"<p>Similarly, pasting denotes forming random subsets of initial data without replacement.</p>"},{"location":"basics/ensemble/#code_2","title":"code","text":"<ul> <li><code>sklearn.ensemble.BaggingClassifier(..., bootstrap=False)</code></li> <li><code>sklearn.ensemble.BaggingRegressor(..., bootstrap=False)</code></li> </ul>"},{"location":"basics/ensemble/#out-of-bag-evaluation","title":"Out-of-Bag Evaluation","text":"<p>Since each predictor has its own unseen part of the dataset, out-of-bag evaluation is a handy way to estimate the overall ensemble score.</p> <p>For [[#Pasting]] ensemble fraction (or number) of out-of-bag samples must be provided explicitly: #code <code>sklearn.ensemble.BaggingClassifier(..., max_samples=0.8)</code>.</p> <p>For [[#Bagging]] ensemble it can be shown that nearly 37% of the training data remains unseen due to replacements, hence no need for explicit data partitioning.</p> <p>\ud83d\udca1 #warning Out-of-bag scores (property <code>oob_score_</code>) is switched off by default. To make it accessible one must provide <code>oob_score=True</code> flag when initializing the <code>BaggingClassifier</code> or <code>BaggingRegressor</code> instance.</p>"},{"location":"basics/ensemble/#random-patches-and-random-subspaces","title":"Random Patches and Random Subspaces","text":"<p>The same logic of [[#Bagging and Pasting]] can be applied for predictor variables instead of (or along with) instances.</p> <p>Both these techniques generally lead to slightly higher [[base/Statistics/Notation#Bias|bias]] but lower [[base/Statistics/Notation#Variance|variance]].</p>"},{"location":"basics/ensemble/#random-patches","title":"Random Patches","text":"<p>Sampling both features and instances is called Random Patches.</p>"},{"location":"basics/ensemble/#code_3","title":"code","text":"<ul> <li><code>sklearn.ensemble.BaggingClassifier(..., max_samples=0.8, bootstrap=True/False, max_features=0.8, bootstrap_features=True)</code></li> <li><code>sklearn.ensemble.BaggingRegressor(..., max_samples=0.8, bootstrap=True/False, max_features=0.8, bootstrap_features=True)</code></li> </ul>"},{"location":"basics/ensemble/#random-subspaces","title":"Random Subspaces","text":"<p>Sampling only features and drawing all instances without replacement is called Random Subspaces.</p>"},{"location":"basics/ensemble/#code_4","title":"code","text":"<ul> <li><code>sklearn.ensemble.BaggingClassifier(..., max_samples=1.0, bootstrap=False, max_features=0.8, bootstrap_features=True/False)</code></li> <li><code>sklearn.ensemble.BaggingRegressor(..., max_samples=1.0, bootstrap=False, max_features=0.8, bootstrap_features=True/False)</code></li> </ul>"},{"location":"basics/ensemble/#boosting","title":"Boosting","text":"<p>-</p>"},{"location":"basics/ensemble/#stacking","title":"Stacking","text":"<p>-</p>"},{"location":"basics/regression/","title":"Regression","text":""},{"location":"basics/regression/#parameters-confidence-intervals","title":"Parameters Confidence Intervals","text":"<p>Confidence intervals for regression coefficients:</p> <ol> <li>Draw $2n$ records from the original dataset (with replacement)</li> <li>Fit regression model and record the coefficients</li> <li>Repeat steps 1-2 $R$ times</li> <li>For $(1-2p)$ confidence interval, get $p$ and $(1-p)$ level quantiles.</li> </ol>"},{"location":"basics/regression/#stepwise-regression","title":"Stepwise Regression","text":"<p>Stepwise regression is the process of searching for the optimal set of predictor variables.</p> <p>\ud83d\udca1 #story It\u2019s common to use the principle of Occam\u2019s razor when building a regression model: a simpler model (i.e. with fewer predictor variables) should be used in preference to a more complicated model, ceteris paribus.</p>"},{"location":"basics/regression/#vanilla-predictor-selection","title":"Vanilla Predictor Selection","text":"<p>Adding and dropping predictor variables based on adjusted R-squared or AIC metrics and statistical significance as you go.</p>"},{"location":"basics/regression/#all-subset-regression","title":"All Subset Regression","text":"<p>All subset regression: searching through each possible combination of predictor variables. Computationally expensive and most prone to overfitting.</p>"},{"location":"basics/regression/#forward-selection","title":"Forward Selection","text":"<p>Forward selection: start with no predictors at all (constant model) and add them one by one greedily (i.e. those having the largest contribution to the metric). The process stops when the contribution is no longer statistically significant.</p>"},{"location":"basics/regression/#backward-selection","title":"Backward Selection","text":"<p>Backward regression (backward elimination): start with all available predictor variables (i.e. the full model) and take away those which aren\u2019t statistically significant until all the predictor variables are statistically significant.</p>"},{"location":"basics/regression/#penalized-regression","title":"Penalized Regression","text":"<p>An alternate approach to stepwise regression is penalized regression: instead of eliminating predictors, it applies the $\\ell_1$ penalty (lasso regression) or $\\ell_2$ penalty (ridge regression) thus reducing coefficients of insignificant or highly correlated predictors.</p>"},{"location":"basics/regression/#categorical-variables-in-regression","title":"Categorical Variables in Regression","text":"<p>Categorical variables are also called factor variables. Binary variables are also called indicator variables.</p> <p>Factor variables are typically translated to dummy variables or one-hot encoded before fitting into the regression model. This is done via <code>pandas.get_dummies(series, drop_first=True)</code>.</p> <p>\ud83d\udca1 One of the categories must be omitted to avoid multicollinearity, i.e. fitting a model with dependent variables. E.g. if factor variable $x$ can take ($p$) values $A$, $B$, or $C$, then if both $x_A$ and $x_B$ are false, $x_C$ must be true. Thus, only $x_A$ and $x_B$ (i.e. $p-1$, which is degrees of freedom) dummy variables should be kept.</p> <p>Factor variables with too many categories can be translated to numeric variables by mapping them to mean/median of the target (e.g. city id can be translated to median city house price) or residual from the target given by model ignoring this variable. Similarly, we can reduce the number of categories by grouping the obtained numeric values, producing new categories. This can be done via <code>pandas.qcut(series, q=4, labels=[\"s\", \"m\", \"l\", \"xl\"])</code>.</p> <p>Ordinal variables (or ordered factor variables) can be treated as numeric variables (or assigned to those according to order) in most cases. This helps to preserve the order.</p>"},{"location":"basics/regression/#interpreting-the-regression-equation","title":"Interpreting the Regression Equation","text":"<p>In statistics, regression is mainly a tool for explanatory modeling, not for prediction. This can be of value for Data Science applications too.</p>"},{"location":"basics/regression/#correlated-predictors","title":"Correlated predictors","text":"<p>Correlated predictors can skew the coefficients to nonsense (i.e. total space, number of living rooms, and number of bathrooms)</p>"},{"location":"basics/regression/#multicollinearity","title":"Multicollinearity","text":"<p>Multicollinearity is the extreme case for correlated variables. It is usually the case when the model is fitted with the same predictor added multiple times by mistake, $p$ instead of $(p-1)$ dummy variables, or nearly perfectly correlated predictors.</p>"},{"location":"basics/regression/#confounding-variables","title":"Confounding variables","text":"<p>Confounding variables are mistakenly omitted variables of high significance. Ignoring them introduces randomness in coefficients and leads to incorrect conclusions.</p>"},{"location":"basics/regression/#interactions","title":"Interactions","text":"<p>Interactions are combinations (usually products) of predictor variables (or main effects). Adding meaningful interactions (e.g. house total space and price region) to the model can significantly improve scores and explainability.</p> <p>\ud83d\udca1 Searching for the proper interaction of predictors can be challenging. Interactions are selected either based on prior experience or via stepwise regression approaches. Another way to account for interactions is switching to a non-linear model, e.g. decision trees, random forests, or XGBoost. Those search for optimal interactions automatically.</p>"},{"location":"basics/regression/#regression-diagnostics","title":"Regression Diagnostics","text":""},{"location":"basics/regression/#outliers-in-residuals","title":"Outliers in Residuals","text":"<p>A general approach to detect outliers is based on boxplot or z-score (i.e. residual divided by the standard error of all residuals). Outliers of residuals can help spot anomalies in data (e.g. units mismatch, misspellings, column swap, etc.), including fraud.</p>"},{"location":"basics/regression/#influential-values","title":"Influential values","text":"<p>Influential values have high leverage on regression, i.e. excluding these records would significantly change regression coefficients.</p> <p>![[assets/img/Statistics/Regression/01.png|300]]</p> <p>There are a few metrics to determine the influence of a single record:</p>"},{"location":"basics/regression/#hat-values","title":"Hat Values","text":"<p>Given that $\\hat{Y}=HY$, where $H$ is the hat matrix, diagonal values (hat-values) larger than $2(p+1)/n$ indicate high leverage records.</p>"},{"location":"basics/regression/#cooks-distance","title":"Cook's Distance","text":"<p>Cook\u2019s distance larger than $4/(n-p-1)$ indicates high leverage records.</p>"},{"location":"basics/regression/#bubble-plot","title":"Bubble Plot","text":"<p>A bubble plot is a scatter plot of hat-values versus residual z-scores with dot size equal to Cook\u2019s distance.</p> <p>![[assets/img/Statistics/Regression/02.png|300]]</p>"},{"location":"basics/regression/#sourcecode-regression-codebubble-plot","title":"sourcecode [[Regression Code#Bubble Plot]].","text":""},{"location":"basics/regression/#heteroskedasticity","title":"Heteroskedasticity","text":"<p>Heteroskedasticity is a difference in residual variance across the range of predicted values, i.e. variance of the residual depends on the predicted value. It can be assessed visually from a scatter plot of predictions versus residuals with spline smoothing</p> <p>![[assets/img/Statistics/Regression/03.png|400]]</p>"},{"location":"basics/regression/#sourcecode-regression-codeheteroskedacticity-plot","title":"sourcecode [[Regression Code#Heteroskedacticity Plot]].","text":""},{"location":"basics/regression/#residuals-distribution","title":"Residuals Distribution","text":"<p>The distribution of residuals, which is a subject of interest for statisticians exclusively, can actually tell much about the model quality. That is, normally distributed residuals indicate that the model is complete, whereas the opposite is a clear sign that the model is missing something.</p>"},{"location":"basics/regression/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>MSE is given by</p> <p>$$ \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m{     \\left(         \\boldsymbol{y}^{(i)}-\\boldsymbol{\\hat{y}}^{(i)}     \\right)^2     } $$</p>"},{"location":"basics/regression/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>RMSE is given by</p> <p>$$ \\text{RMSE} = \\sqrt{     \\frac{1}{m} \\sum_{i=1}^m{         \\left(             \\boldsymbol{y}^{(i)}-\\boldsymbol{\\hat{y}}^{(i)}         \\right)^2     } } $$</p>"},{"location":"basics/regression/#residual-standard-error-rse","title":"Residual Standard Error (RSE)","text":"<p>RSE is given by</p> <p>$$ \\text{RSE} = \\sqrt{     \\frac{1}{m - n - 1} \\sum_{i=1}^m{         \\left(             \\boldsymbol{y}^{(i)}-\\boldsymbol{\\hat{y}}^{(i)}         \\right)^2     } } $$</p> <p>the only difference is that the denominator is the degrees of freedom (i.e. number of records minus number of predictor variables minus one) instead of number of records.</p>"},{"location":"basics/regression/#coefficient-of-determination-r-squared","title":"Coefficient of determination (R-squared)","text":"<p>R-squared is given by</p> <p>$$  R^2 = 1 - \\frac{\\sum_{i=1}^m{         \\left(             \\boldsymbol{y}^{(i)}-\\boldsymbol{\\hat{y}}^{(i)}         \\right)^2         }     }     {\\sum_{i=1}^m{         \\left(             \\boldsymbol{y}^{(i)}-\\boldsymbol{\\overline{y}}^{(i)}         \\right)^2     } } $$</p> <p>The denominator is proportional to the target variance. Hence R-squared indicates the fraction of target variance accounted in the model.</p>"},{"location":"basics/regression/#adjusted-r-squared","title":"Adjusted R-squared","text":"<p>Adjusted R-squared penalizes the model with too many predictor variables:</p> <p>$$ R_{adj}^2 = 1 - \\left( 1 - R^2 \\right) \\frac{m - 1}{m - n - 1} $$</p>"},{"location":"basics/regression/#akaikes-information-criterion-aic","title":"Akaike\u2019s information criterion (AIC)","text":"<p>AIC takes into account model\u2019s complexity</p> <p>$$ \\text{AIC} = 2n + m \\log{(\\text{MSE})} $$</p> <p>so that $n$ more predictor variables would be penalized by $2n$.</p>"},{"location":"basics/regression/#t-statistic","title":"T-Statistic","text":"<p>The t-statistic is calculated for each model's parameter </p> <p>$$ t_b=\\frac{         \\boldsymbol{\\theta}^{(i)}     }     {\\text{SE}\\left(\\boldsymbol{\\theta}^{(i)}\\right)} $$</p> <p>where $\\text{SE}$ is [[base/Statistics/Data and Sampling#Standard Error|Standard Error]] of parameter. It can be assessed similarly to [[base/Statistics/Regression#Parameters Confidence Intervals| parameters confidence intervals]] by [[Data and Sampling#Bootstrap|bootstrapping]] the data and refitting the model.</p> <p>Since t-statistic is the mirror image of the p-value, the higher the value of t-statistic, the more statistically significant the predictor variable is.</p>"},{"location":"basics/statistical_tests/","title":"Statistical Tests","text":""},{"location":"basics/statistical_tests/#ab-testing","title":"A/B Testing","text":""},{"location":"basics/statistical_tests/#empty-add-here-a-detailed-description-from-the-naked-statistics","title":"empty Add here a detailed description from the \u201cNaked Statistics\u201d.","text":""},{"location":"basics/statistical_tests/#statistical-significance-and-p-values","title":"Statistical Significance and P-Values","text":"<p>Significance tests are eliminating two major problems: -   Humans tend to underestimate the scope of natural random effects (\"black swan\" effect) -   Humans tend to misinterpret random events as significant patterns</p>"},{"location":"basics/statistical_tests/#p-value","title":"P-Value","text":"<p>Given a chance that the null hypothesis is true, the p-value is the probability of results obtained.</p>"},{"location":"basics/statistical_tests/#significance-level","title":"Significance Level","text":"<p>Significance level $\\alpha$ is the upper bound for the likelihood of observing (at least as extreme) pattern of data if the null hypothesis is true.</p> <p>\ud83d\udca1 The correct way to use these notions is, e.g.: \u201cReject the null hypothesis at the .05 level\".</p>"},{"location":"basics/statistical_tests/#type-i-error","title":"Type I Error","text":"<p>Type I error (false positive): concluding effect is real when it's a product of chance.</p>"},{"location":"basics/statistical_tests/#type-ii-error","title":"Type II Error","text":"<p>Type II error (false negative): concluding effect is a product of chance when it's real.</p>"},{"location":"basics/statistical_tests/#permutation-tests","title":"Permutation Tests","text":"<p>Permutation tests are more of a hands-on way to estimate whether the observed difference of the value in an A/B test is a product of chance.</p> <p>This algorithm is not available in any of python libraries, yet it\u2019s easy to implement:</p> <ol> <li>Combine the results from all treatment groups</li> <li>Draw new samples and record new values of interest</li> <li>Repeat step 2 $R$ times</li> <li>Assess results distribution (via histogram, bar, etc.)</li> </ol> <p>There are other variations as well:</p> <ul> <li>An exhaustive permutation test (i.e. drawing conclusions from all possible permutations)</li> <li>Bootstrap permutation tests (with replacement)</li> </ul> <p>\ud83d\udca1 T-test is a statistic-based replacement for permutation test: #code <code>scipy.stats.ttest_ind(x, y, equal_var=False, permutations=1)</code></p>"},{"location":"basics/statistical_tests/#vast-search-effect-and-multiple-testing","title":"Vast Search Effect and Multiple Testing","text":"<p>The more predictor variables you test, the higher the chance of type I error. This is called the vast search effect. E.g. probability of observing one or more type I error when assessing 20 predictor variables at .05 significance level:</p> <p>$$ P=1 - 0.95^{20} = 1 - 0.36=64 \\% $$</p> <p>The same is true for multiple statistical comparisons (e.g. is A different from B, C, D, etc.?) and multiple models or, in general, multiple testing.</p> <p>\ud83d\udca1 This can be summarized in the quote: \u201cIf you torture the data long enough, sooner or later it will confess\u201d.</p> <p>In order to minimize false discovery rate, different techniques can be applied:</p> <ul> <li>adjustment of the p-value (typically divided by the number of comparisons or models)</li> <li>hold-out set (typically for regression tasks)</li> </ul>"},{"location":"basics/statistical_tests/#anova","title":"ANOVA","text":"<p>Analysis of variance (ANOVA) is a statistical test to measure the significance of differences among multiple treatment groups. The value of interest measured must be numeric.</p> <p>ANOVA is used when we are interested if the overall variance between all the treatment groups is a product of chance. That is, no attention is paid to variance between individual treatment groups.</p> <p>This algorithm is not available in any of python libraries, yet it\u2019s easy to implement. It differs from the permutation test just by one additional step:</p> <ol> <li>Combine all the data together</li> <li>Draw new samples and record new values of interest</li> <li>Record the variance between results for all the groups</li> <li>Repeat the steps (2-3) $R$ times</li> <li>The p-value is the fraction of results exceeding the original variance.</li> </ol> <p>\ud83d\udca1 A statistic-based replacement for ANOVA is to f-statistic.</p>"},{"location":"basics/statistical_tests/#pearsons-chi-squared-test","title":"Pearson\u2019s Chi-Squared Test","text":"<p>Chi-square test is designed to test the observed distribution of categorical data against the expected (usually uniform) distribution. It is widely used for independence tests.</p> <p>This test requires data in a form of $r \\times c$ contingency table, i.e. table of frequency distribution of the variables. Degrees of freedom can be calculated like:</p> <p>$$ \\text{dof} = (r - 1)(c - 1) $$</p> <p>The chi-square statistic is then computed as follows:</p> <p>$$ \\chi^2 = \\sum_i^r \\sum_j^c \\frac{(O - E)^2}{E} $$</p> <p>where $O$ is the observed value, and $E$ is the expected value.</p> <p>\ud83d\udca1 #warning When value counts are extremely low (the rule of thumb is five or fewer), it\u2019s best to use permutation test or exhaustive permutation test\u2014Fisher\u2019s exact test.</p>"},{"location":"basics/statistical_tests/#multi-arm-bandit-algorithms","title":"Multi-Arm Bandit Algorithms","text":"<p>Unlike classic [[#A B Testing|A/B tests]], multi-arm bandit algorithms allow adjusting size of treatment groups on the fly to maximize outcomes.</p> <p>Typically used in web testing and other applications where the main point is not to prove that the distinctions between the treatments are statistically significant but to maximize the outcomes (e.g. conversion rates).</p> <p>One of possible algorithms may be like this:</p> <ul> <li>Randomly assign a new specimen to any of the treatment groups with probability of $\\varepsilon$</li> <li>Assign a new specimen to the highest scoring treatment group with probability of $(1-\\varepsilon)$</li> <li>Adjust $\\varepsilon$ to the results accordingly</li> </ul> <p>\ud83d\udca1 When $\\varepsilon=0$ the algorithm becomes a classic A/B test, and when $\\varepsilon=1$ the algorithm becomes greedy (i.e. always choosing the best option based on fixed number of previous scores).</p> <p>Another approach is called Thompson\u2019s sampling. It uses Bayesian theorem to maximize the probability of choosing the best treatment for each specimen.</p>"},{"location":"basics/statistical_tests/#power-and-sample-size","title":"Power and Sample Size","text":"<p>There are four dependent characteristics in a test:</p> <ul> <li>Sample size</li> <li>Effect size</li> <li>Power</li> <li>Significance level</li> </ul> <p>Typically the characteristic of interest is the sample size.</p>"},{"location":"basics/statistical_tests/#effect-size","title":"Effect size","text":"<p>Effect size is the minimum difference of the value of interest between two treatment groups to be proved as statistically significant.</p>"},{"location":"basics/statistical_tests/#power","title":"Power","text":"<p>Power is the probability of detecting a given effect size within a sample of given size and variability.</p>"},{"location":"basics/statistical_tests/#examples","title":"Examples","text":""},{"location":"basics/statistical_tests/#simple-statistical-significance-test-example","title":"Simple Statistical Significance Test #example","text":"<p>Do printers have higher diastolic blood pressure levels than farmers?</p> Occupation Observations Mean blood pressure (mmHg) Std (mmHg) Printer 72 86 8.5 Farmer 48 82 8.2"},{"location":"basics/statistical_tests/#solution","title":"Solution","text":"<p>Set up the null hypothesis and the alternate hypothesis. Null hypothesis: there is no difference in diastolic blood pressure levels between printers and farmers. Alternate hypothesis: printers and farmers have different blood pressure.</p> <p>Set up significance level ($\\alpha$) of .05 for rejecting the null hypothesis.</p> <p>According to [[Data and Sampling#Standard error on difference|this formula]] standard error for difference in means is:</p> <p>$$ \\text{SE (diff)} = \\sqrt{\\frac{8.5^2}{72} + \\frac{8.2^2}{48}} = 1.55 $$</p> <p>The observed difference in means in SE scale is:</p> <p>$$ z = \\frac{86 - 82}{1.55} = 2.58 $$</p> <p>The p-value is given by <code>2 * (1 - scipy.stats.norm.cdf(2.58))</code>: 0.01.</p> <p>Thus the null hypothesis can be rejected at .05 level.</p>"},{"location":"basics/statistical_tests/#permutation-test-example","title":"Permutation Test #example","text":"<p>Does any of the headlines A, B, or C really attract readers the most?</p> Action Headline A Headline B Headline C Total Click 14 8 12 34 No click 986 992 988 2,966 Total 1,000 1,000 1,000 3,000"},{"location":"basics/statistical_tests/#solution_1","title":"Solution","text":"<p>The null hypothesis is that actions are uniformly distributed. If so</p> Action Headline A Headline B Headline C Total Click 11.33 11.33 11.33 34 No click 988.67 988.67 988.67 2,966 Total 1,000 1,000 1,000 3,000 <p>Pearson's residual for each row</p> Action Headline A Headline B Headline C Total Click 0.792 -0.990 0.198 0 No click -0.085 0.106 -0.021 0 Total 0 0 0 0 <p>Compute the chi-squared statistic for the entire table</p> <p>$$ \\chi = \\sum_i^r \\sum_j^c R^2=1.67 $$</p> <p>Make a permutation set of 34 positives (clicks) and 2,966 negatives (no clicks). Apply the steps above to random $R$ random permutations and draw the $\\chi$ values. The p-value is the fraction of results exceeding the original $\\chi$ value. In this case:</p> <p>$$ p=0.48 $$</p> <p>Thus the null hypothesis cannot be rejected at .05 level.</p>"},{"location":"basics/statistical_tests/#pearsons-chi-squared-test-example-1","title":"Pearson\u2019s Chi-Squared Test #example 1","text":"<p>The problem statement is the same as for [[#Permutation Test example| headline example]] above.</p>"},{"location":"basics/statistical_tests/#solution_2","title":"Solution","text":"<pre><code>&gt;&gt;&gt; from scipy.stats import chi2_contingency\n&gt;&gt;&gt; observed = [\n&gt;&gt;&gt;     [14, 8, 12, 34], \n&gt;&gt;&gt;     [986, 992, 988, 2966],\n&gt;&gt;&gt;     [1000, 1000, 1000, 3000],\n&gt;&gt;&gt; ]\n&gt;&gt;&gt; chisq, pvalue, dof, expected = chi2_contingency(observed)\n&gt;&gt;&gt; round(chisq, 2)\n1.67\n&gt;&gt;&gt; round(pvalue, 2)\n0.43\n&gt;&gt;&gt; dof\n2\n&gt;&gt;&gt; expected\narray([[  11.33333333,   11.33333333,   11.33333333,   34.        ],\n       [ 988.66666667,  988.66666667,  988.66666667, 2966.        ],\n       [1000.        , 1000.        , 1000.        , 3000.        ]])\n</code></pre>"},{"location":"basics/statistical_tests/#pearsons-chi-squared-test-example-2","title":"Pearson\u2019s Chi-Squared Test #example 2","text":"<p>Did the researcher fabficate her results?  Here\u2019s 315 interior digits drawn from the paper, (i.e. excluding the first and the last digits, which are usually not random):</p> Digit 0 1 2 3 4 5 6 7 8 9 Frequency 14 71 7 65 23 19 12 45 53 6 <p>BTW, this is a real #story.</p>"},{"location":"basics/statistical_tests/#solution_3","title":"Solution","text":"<p>Hence chi-squared test requires data to be at least 2 dimensional with at least 2 rows and 2 columns, we need to reformat the initial table</p> Digit 0 1 2 3 4 5 6 7 8 9 Total Present 14 71 7 65 23 19 12 45 53 6 315 Not present 301 244 308 250 292 296 303 270 262 309 2,835 Total 315 315 315 315 315 315 315 315 315 315 3,150 <p>Apply a chi-squared test for this contingency table</p> <pre><code>&gt;&gt;&gt; from scipy.stats import chi2_contingency\n&gt;&gt;&gt; observed = [\n&gt;&gt;&gt;     [14, 71, 7, 65, 23, 19, 12, 45, 53, 6, 315],\n&gt;&gt;&gt;     [301, 244, 308, 250, 292, 296, 303, 270, 262, 309, 2835],\n&gt;&gt;&gt;     [315, 315, 315, 315, 315, 315, 315, 315, 315, 315, 3150],\n&gt;&gt;&gt; ]\n&gt;&gt;&gt; chisq, pvalue, dof, expected = chi2_contingency(observed)\n&gt;&gt;&gt; f\"{p:.2e}\"\n'1.94e-30'\n</code></pre>"},{"location":"basics/statistical_tests/#sample-size-estimation-example","title":"Sample Size Estimation #example","text":"<p>Set up an A/B test for adds. The current add (the control group) has conversion rate of 1.1%. It\u2019s supposed that a new add should be at least 10% better than the old one (i.e. at least 1.21%). Given that, how many samples must be taken to observe statistically significant difference at least 80% of the time?</p>"},{"location":"basics/statistical_tests/#solution_4","title":"Solution","text":"<p>This can be done via <code>statsmodels</code> package:</p> <pre><code>from statsmodels.stats.proportion import proportion_effectsize\nfrom statsmodels.stats.power import TTestIndPower\n\neffect_size = proportion_effectsize(0.0121, 0.011)\nanalysis = TTestIndPower()\nresult = analysis.solve_power(\n    effect_size=effect_size,\n    alpha=0.05,\n    power=0.8,\n    alternative='larger',\n)\nprint(round(result))\n</code></pre> <p>The result is 116602.</p>"},{"location":"linear_models/lda/","title":"Linear Discriminant Analysis","text":"<p>\ud83d\udca1 #story Discriminant analysis is the earliest statistical classifier; it was introduced by R. A. Fisher in 1936 in an article published in the Annals of Eugenics journal.</p> <p>While discriminant analysis encompasses several techniques, the most commonly used is linear discriminant analysis (LDA).</p> <p>LDA searches for optimal linear decision boundaries between the classes so that sum of squares between the groups is as high as possible while the sum of squares within the groups is as low as possible.</p> <p>Because of LDA decision boundaries can be used as new axes, LDA is also widely used for dimensionality reduction.</p> <p>\ud83d\udca1 Despite LDA is designed for normally distributed numerical predictors, it can be used with slightly non-normal distributed and binary variables as well.</p>"},{"location":"linear_models/lda/#synthetic-data-example","title":"Synthetic Data #example","text":"<p>Classify random samples of two normal distributions with mean 3 (y=0) and 5 (y=1):</p> <p>![[assets/img/Machine Learning/Linear Discriminant Analysis/01.png|600]]</p>"},{"location":"linear_models/lda/#solution","title":"Solution","text":"<pre><code>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport plotly.graph_objects as go\nimport numpy as np\n\nSIZE = 50\n\nX1 = np.random.normal(loc=2, size=(SIZE, 2))\nX2 = np.random.normal(loc=5, size=(SIZE, 2))\n\ny1 = np.zeros(SIZE)\ny2 = np.ones(SIZE)\n\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\nmodel = LinearDiscriminantAnalysis()\nmodel.fit(X, y)\npredict = model.predict_proba(X)\n\ncenter = np.mean(model.means_, axis=0)\ncoef = -float(model.scalings_[0] / model.scalings_[1])\nintercept = float(center[1] - center[0] * coef)\n\ngo.Figure(\n    data=(\n        go.Scatter(\n            x=X[:, 0],\n            y=X[:, 1],\n            mode=\"markers\",\n            marker=dict(\n                size=10,\n                color=predict[:, 0],\n                colorscale=\"Tropic\",\n                colorbar=dict(title=\"class probability\"),\n                showscale=True,\n            ),\n        ),\n        go.Scatter(\n            x=[np.min(X[:, 0]), np.max(X[:, 0])],\n            y=[\n                (np.min(X[:, 0]) - intercept) / coef,\n                (np.max(X[:, 0]) - intercept) / coef,\n            ],\n            line=dict(width=3, color=\"black\"),\n            mode=\"lines\",\n        ),\n    ),\n    layout=dict(\n        height=500,\n        width=700,\n        title_text=\"LDA Example\",\n        xaxis=dict(title_text=\"x1\"),\n        yaxis=dict(title_text=\"x2\"),\n        font=dict(size=16),\n        showlegend=False,\n    ),\n).write_html(\"1.html\")\n</code></pre>"},{"location":"linear_models/linear_classification/","title":"Linear Classification","text":""},{"location":"linear_models/linear_classification/#logistic-regression","title":"Logistic Regression","text":"<p>Logistic Regression is a direct extension of the Linear Regression with its simplicity and high speed for classification tasks. Logistic Regression returns the logistic of the result</p> <p>$$ \\hat{p}=\\sigma(\\boldsymbol{\\theta}^T\\boldsymbol{x}), $$</p> <p>where $\\sigma$ is a standard logistic function (or sigmoid)</p> <p>$$ \\sigma(t)=\\frac{1}{1 + \\exp{(-t)}}. $$</p> <p>Thus, the output is scaled to the $[0;1]$ range of the probability that a particular instance is positive ($y=1$).</p> <p>Output of the original regression equation can be expressed as a logit function (or logarithm of the odds, which is the inverse of the logistic function)</p> <p>$$ \\boldsymbol{\\theta}^T\\boldsymbol{x}=\\sigma^{-1}(p)=\\ln{(\\text{odds}(\\hat{p}))}=\\ln{\\left(\\frac{p}{1-p}\\right)} $$</p> <p>The cost function for classification tasks is known as cross entropy loss (or log loss). In the case of binary classification, it's called binary cross entropy and is given by</p> <p>$$ J(\\boldsymbol{\\theta})=-\\frac{1}{m}\\sum_{i=1}^m{\\left(y^{(i)}\\log{\\hat{p}^{(i)}}+\\left(1-y^{(i)}\\right)\\log{\\left(1-\\hat{p}^{(i)}\\right)}\\right)}. $$</p> <p>Extreme Values Example</p> <p>Imagine that the ground truth $y=1$, while the predicted value $\\hat{p} \\approx 0$. The penalty for this sample is then close to $-\\log{0}$, which is $\\infty$. The same is true for the opposite case, where the ground truth is equal to 0 and the predicted value is close to 1.</p> <p>There is no known closed-form solution for computing the value of $\\boldsymbol{\\theta}$ that minimizes binary cross entropy. However, just like MSE, cross entropy is a convex function. Thus, Batch Gradient Descent is guaranteed to find the optimal solution within the specified tolerance.</p> <p>The cross entropy gradient $\\nabla_{\\boldsymbol{\\theta}}{J(\\boldsymbol{\\theta})}$ is calculated in the same way as for MSE and is given by:</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{J(\\boldsymbol{\\theta})}= \\frac{2}{m}\\boldsymbol{X}^T\\left(\\sigma\\left({\\boldsymbol{X}\\boldsymbol{\\theta}}\\right)-\\boldsymbol{y}\\right). $$</p> <p>Scikit-Learn Implementation</p> <p>The <code>LogisticRegression</code> , the <code>scikit-learn</code> implementation of logistic regression, comes with the $\\ell_2$ penalty turned on by default, allowing you to adjust the strength of the regularization. Read more about regularization in the Regularization manual.</p> <p>Categorical Features</p> <p>As with Linear Regression, all categorical variables must first be one-hot encoded, omitting one of the classes.</p>"},{"location":"linear_models/linear_classification/#softmax-regression","title":"Softmax Regression","text":"<p>The logistic regression model can be generalized to support multiple classes directly, without the need to train one-versus-one (OvO) or one-versus-rest (OvR) classifiers. The resulting probability that an instance belongs to class $k$ is given by</p> <p>$$ \\hat{p}_k=\\text{softmax}(\\boldsymbol{s})_k=\\frac{\\exp{(\\boldsymbol{s}_k)}}{\\sum_i exp{(\\boldsymbol{s}_i)}} $$</p> <p>where $\\boldsymbol{s}$ is a vector containing the values of each class $k$ for the instance $\\boldsymbol{x}$</p> <p>$$ \\boldsymbol{s}=\\boldsymbol{\\Theta}\\boldsymbol{x}. $$</p> <p>All class-wise parameter vectors $\\boldsymbol{\\theta}^{(k)}$ together form a parameter matrix $\\boldsymbol{\\Theta}$.</p> <p>Similar to binary classification, the cost function used is cross-entropy, but in a generalized version known as categorical cross-entropy</p> <p>$$ J(\\boldsymbol{\\Theta})=-\\frac{1}{m}\\sum_{i=1}^m{\\sum_{k=1}^l{y_k^{(i)}\\log{\\left(\\hat{p}_k^{(i)}\\right)}}} $$</p> <p>where $l$ is the number of classes.</p> <p>The gradient vector with respect to the $\\boldsymbol{\\theta}^{(k)}$ parameter vector $\\nabla_{\\boldsymbol{\\theta}^{(k)}}{J(\\boldsymbol{\\theta})}$ is calculated similarly to the gradient of the MSE and is given by</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}^{(k)}}{J(\\boldsymbol{\\Theta})}= \\frac{2}{m}\\boldsymbol{X}^T\\left(\\text{softmax}\\left({\\boldsymbol{X}\\boldsymbol{\\theta}}\\right)-\\boldsymbol{y}\\right). $$</p> <p>Scikit-Learn Implementation</p> <p>The <code>LogisticRegression</code>  implementation in <code>scikit-learn</code> uses One-versus-Rest wrapper for multiclass tasks instead.</p>"},{"location":"linear_models/linear_regression/","title":"Linear Regression","text":"<p>Linear Regression is defined by</p> <p>$$ \\hat{y} = \\boldsymbol{\\theta}^T \\boldsymbol{x}, $$</p> <p>where $\\hat{y}$ represents the predicted value, $\\boldsymbol{\\theta}$ is a $(n, 1)$ vector of model weights, $\\boldsymbol{x}$ is a $(n, 1)$ vector of features, and $n$ is the number of features.</p> <p>The mean squared error $\\text{MSE}(\\boldsymbol{\\theta})$ is commonly used as the cost function</p> <p>$$ \\text{MSE}(\\boldsymbol{\\theta})=\\frac{1}{m} \\sum_{i=1}^m{(\\boldsymbol{\\theta}^T \\boldsymbol{x}^{(i)} - y^{(i)})^2} = \\frac{1}{m}|\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}|_2^2, $$</p> <p>where $m$ represents the number of training samples, $\\boldsymbol{X}$ is an $(m, n)$ matrix containing the training samples.</p> <p>Euclidean Norm</p> <p>The $|| x ||_2$ notation represents the Euclidean (or L2) norm of the vector. Similarly, $|| x ||_2^2$ represents the squared Euclidean norm.</p> <p>There are 2 main solution approaches:</p> <ul> <li>Closed-form solution computes the optimal parameters directly.</li> <li>Gradient descent iteratively finds near-optimal parameters by fitting either the entire training set or its batches.</li> </ul> <p>Both approaches are described in the following sections.</p> <p>Categorical Features</p> <p>Regardless of the implementation you use, note that all categorical variables must first be one-hot encoded, omitting one of the classes.</p>"},{"location":"linear_models/linear_regression/#closed-form-solution","title":"Closed-Form Solution","text":"<p>To minimize the cost function $\\text{MSE}(\\boldsymbol{\\theta})$, we seek the point at which its gradient vanishes</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}=0, $$</p> <p>$$ \\frac{1}{m}\\nabla_{\\boldsymbol{\\theta}} ||\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}||_2^2=0. $$</p> <p>Considering that $||\\boldsymbol{X}||_2^2=\\boldsymbol{X}^T\\boldsymbol{X}$, we have</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y})^T(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y})}=0, $$</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{(\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{y}-\\boldsymbol{y}^T\\boldsymbol{X}\\boldsymbol{\\theta}+\\boldsymbol{y}^T\\boldsymbol{y})}=0. $$</p> <p>Using scalar triple product property gives</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{(\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}-2\\boldsymbol{\\theta}^T\\boldsymbol{X}^T\\boldsymbol{y}+\\boldsymbol{y}^T\\boldsymbol{y})}=0. $$</p> <p>Finally, applying the gradient gives</p> <p>$$ 2\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\theta}-2\\boldsymbol{X}^T\\boldsymbol{y}=0, $$</p> <p>$$ \\boxed{\\boldsymbol{\\theta}=(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}}. $$</p> <p>The resultant equation is recognized as the normal equation.</p> <p>Computational Complexity</p> <p>Due to the computational complexity, which is about $n^3$ for finding the inverse matrix alone, the closed-form solution may be unsuitable for large $m$ and $n$ (i.e., large number of instances and features, respectively)..</p> Example: Fit Noisy Linear Function <p>Fit the $y=3 \\cdot x + 4$ equation given 100 noisy samples.</p> <p>The problem can be solved directly with <code>numpy</code> with a little trickery:</p> <pre><code>import numpy as np\n\nnp.random.seed(42)\nsize = 100\n\nnoise = 0.25 * np.random.randn(size, 1)\nx1 = np.random.rand(size, 1)\nx2 = np.ones_like(x1)  # (1)\nX = np.c_[x1, x2]\ny = 3 * x1 + 4 + noise\n\ntheta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\nprint(f\"The equation is: y={theta[0, 0]:.2f} * x + {theta[1, 0]:.2f}.\")\n</code></pre> <ol> <li>Although there is only one argument to the function, we need to use 2 features (one being the argument and the other being all ones) to find the intercept.</li> </ol> <p>Or more traditionally, with <code>scikit-learn</code>:</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(42)\nsize = 100\n\nnoise = 0.25 * np.random.randn(size, 1)\nX = np.random.rand(size, 1)\ny = 3 * X + 4 + noise\n\nmodel = LinearRegression()\nmodel.fit(X, y)\nprint(f\"The equation is: y={model.coef_[0, 0]:.2f} * x + {model.intercept_[0]:.2f}.\")\n</code></pre>"},{"location":"linear_models/linear_regression/#gradient-descent","title":"Gradient Descent","text":"<p>Gradient descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems.</p> <p>Mind Scaling</p> <p>When using gradient descent, all inputs must be on the same scale for good convergence.</p> <p>Gradient descent begins with random initialization of model parameters and tweaks the model\u2019s parameter vector $\\boldsymbol{\\theta}$ opposite to the gradient of the cost function at each iteration</p> <p>$$ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})} $$</p> <p>where $\\eta$ is the learning rate.</p> <p>The gradient vector $\\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}$ is given by</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}= \\begin{pmatrix} \\frac{\\partial}{\\partial \\theta_0} \\text{MSE}(\\boldsymbol{\\theta}) \\ \\vdots \\ \\frac{\\partial}{\\partial \\theta_n} \\text{MSE}(\\boldsymbol{\\theta}) \\end{pmatrix} = \\frac{2}{m}\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}) $$</p> <p>(it was derived here). Thus, partial derivatives are given by</p> <p>$$ \\frac{\\partial}{\\partial \\theta_j}\\text{MSE}(\\boldsymbol{\\theta})=\\frac{2}{m}\\sum_{i=1}^m{\\left(\\boldsymbol{\\theta}^T\\boldsymbol{x}^{(i)}-y^{(i)}\\right)x_j^{(i)}} $$</p> <p>The iterations continue until the norm of the gradient becomes less than the specified tolerance value $\\epsilon$</p> <p>$$ |\\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})}|&lt;\\epsilon $$</p> <p>The cost function $\\text{MSE}(\\boldsymbol{\\theta})$ is convex, meaning that gradient descent is guaranteed to find the global minimum within the specified tolerance.</p> <p>There are 3 types of Gradient Descent:</p> <ul> <li>Batch gradient descent (shown above): the entire training set is used to compute the gradient. Guaranteed to find optimal parameters for convex cost functions. Gets slow with large datasets and lots of features and does not support out-of-core computations.</li> <li>Mini-batch gradient descent: a single batch is used to compute the gradient. Scale well to huge datasets and supports out-of-core computations.</li> <li>Stochastic gradient descent: a single instance is used to compute the gradient. May require learning rate schedule for good convergence. Scale well to huge datasets and supports out-of-core computations.</li> </ul> <p>Computational Complexity</p> <p>The complexity of gradient descent is $O(m \\times n)$, which is drastically lower than that of a closed-form solution.</p> Example: Fit Noisy Linear Function <p>Fit the $y=3 \\cdot x + 4$ equation given 100 noisy samples.</p> <p>Batch and mini-batch gradient descent implementations are not directly available in scikit-learn. However, it's easy to implement:</p> <pre><code>import numpy as np\n\nnp.random.seed(42)\nsize = 100\n\nnoise = 0.25 * np.random.randn(size, 1)\nx1 = np.random.rand(size, 1)\nx2 = np.ones_like(x1)  # (1)\nX = np.c_[x1, x2]\ny = 3 * x1 + 4 + noise\n\neta = 0.1\nmax_iter = 1000\nm, n = X.shape\n\ntheta = np.random.randn(n, 1)\n\nfor _ in range(max_iter):\n    grad = 2 / m * np.dot(X.T, np.dot(X, theta) - y)\n    theta -= eta * grad\n\nprint(f\"The equation is: y={theta[0, 0]:.2f} * x + {theta[1, 0]:.2f}.\")\n</code></pre> <p>Stochastic gradient descent is available directly:</p> <pre><code>import numpy as np\nfrom sklearn.linear_model import SGDRegressor\n\nnp.random.seed(42)\nsize = 100\n\nnoise = 0.25 * np.random.randn(size, 1)\nX = np.random.rand(size, 1)\ny = 3 * X + 4 + noise\n\nmodel = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nmodel.fit(X, y)\nprint(f\"The equation is: y={model.coef_[0]:.2f} * x + {model.intercept_[0]:.2f}.\")\n</code></pre> <p>Mini-batch gradient descent is not available in scikit-learn.</p>"},{"location":"linear_models/linear_regression/#other-types-of-regression","title":"Other types of regression","text":""},{"location":"linear_models/linear_regression/#polynomial-regression","title":"Polynomial Regression","text":"<p>Linear models, such as <code>LinearRegression</code>  and <code>SGDRegressor</code> , can be extended to higher powers of input features with <code>PolynomialFeatures</code> . It returns all possible permutations of a given degree, for example, $x_1^2$, $x_1 x_2$, and $x_2^2$ for two features $x_1$, $x_2$ and a degree of 2.</p>"},{"location":"linear_models/linear_regression/#spline-regression","title":"Spline Regression","text":"<p>Spline regression can produce even more sophisticated relations by using a series of polynomial segments joined at specified knots. It is available with <code>SplineTransformer</code> . The resulting segments can then be fit into regression model of one\u2019s choice</p> <p>Example</p> <p>See the example  of using polynomial regression with spline interpolation in <code>scikit-learn</code> documentation.</p>"},{"location":"linear_models/linear_regression/#generalized-linear-models","title":"Generalized Linear Models","text":"<p>The process of specifying knots in splines can be automated using Generalized Additive Models (GAM) regression. GAMs are available in the <code>pygam</code>  package.</p> <p>Example</p> <p>See the example  of GAM regression in <code>pygam</code> documentation.</p>"},{"location":"linear_models/regularization/","title":"Regularization","text":""},{"location":"linear_models/regularization/#biasvariance-trade-off","title":"Bias/Variance Trade-off","text":"<p>Model\u2019s generalization error can be represented as sum of three errors:</p> <ul> <li>[[base/Statistics/Notation#Bias|Bias]]</li> <li>[[base/Statistics/Notation#Variance|Variance]]</li> <li>Irreducible error is the part of generalization error due to the noise in the data.</li> </ul> <p>Increasing model\u2019s complexity typically decreases its bias and increases variance and vice versa.</p> <p>One approach to regularization is weight decay (i.e. penalizing the model for large weight values). It is used in the models below:</p> <ul> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Elastic Net</li> </ul> <p>Note</p> <p>Ridge Regression is a good default choice. Lasso and Elastic Net can be used instead when it\u2019s not obvious what features are the most important hence they both tend to eliminate useless features.</p>"},{"location":"linear_models/regularization/#ridge-regression","title":"Ridge Regression","text":"<p>Ridge regression (also called Tikhonov regularization) adds a regularization term equal to half square of the $\\ell_2$ norm of the weight vector to the $\\text{MSE}$ cost function</p> <p>$$ J(\\boldsymbol{w})=\\text{MSE}+\\frac{\\alpha}{2}|\\boldsymbol{w}|_2^2=\\text{MSE}+\\frac{\\alpha}{2}\\boldsymbol{w}^T\\boldsymbol{w} $$</p> <p>The normal equation thus changes to</p> <p>$$ \\boldsymbol{\\theta}=(\\boldsymbol{X}^T\\boldsymbol{X}+\\alpha\\boldsymbol{A})^{-1}\\boldsymbol{X}^T\\boldsymbol{y} $$</p> <p>The gradient vector of the cost function changes to</p> <p>$$ \\nabla_{\\boldsymbol{\\theta}}{\\text{MSE}(\\boldsymbol{\\theta})} = \\frac{2}{m}\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{\\theta}-\\boldsymbol{y}) + \\alpha\\boldsymbol{w} $$</p> <ul> <li><code>sklearn.linear_model.Ridge</code></li> <li><code>sklearn.linear_model.SGDRegressor(penalty=\"l2\")</code></li> </ul>"},{"location":"linear_models/regularization/#lasso-regression","title":"Lasso Regression","text":"<p>Least Absolute Shrinkage and Selection Operator (LASSO) Regression adds a regularization term equal to $\\ell_1$ norm of the weight vector to the $\\text{MSE}$ cost function</p> <p>$$ J(\\boldsymbol{w})=\\text{MSE}+\\alpha|\\boldsymbol{w}|_1 $$</p> <p>Warning</p> <p>Unlike Ridge Regression, Lasso Regression tends to eliminate the weights of the least important features.</p> <ul> <li><code>sklearn.linear_model.Lasso</code></li> <li><code>sklearn.linear_model.SGDRegressor(penalty=\"l1\")</code></li> </ul>"},{"location":"linear_models/regularization/#elastic-net","title":"Elastic Net","text":"<p>Elastic net adds two regularization terms at once corresponding to $\\ell_1$ and $\\ell_2$ norms of the weight vector</p> <p>$$ J(\\boldsymbol{w})=\\text{MSE}+r\\alpha|\\boldsymbol{w}|_1+\\frac{1-r}{2}\\alpha|\\boldsymbol{w}|_2^2 $$</p> <p>where $r$ is the ratio between the terms (e.g. $r=1$ is equivalent to Lasso Regression, while $r=0$ is equivalent to Ridge Regression).</p> <p>Note</p> <p>Elastic Net\u2019s behaviour is similar to Lasso Regression.</p> <ul> <li><code>sklearn.linear_model.ElasticNet(l1_ratio=0.5)</code></li> <li><code>sklearn.linear_model.SGDRegressor(penalty=\"elasticnet\", l1_ratio=0.5)</code></li> </ul>"},{"location":"other_models/hierarchical_clustering/","title":"Hierarchical Clustering","text":"<p>Hierarchical clustering does not need the number of clusters to be specified. It is computationally expensive, as it requires pairwise distance of the entire dataset.</p>"},{"location":"other_models/hierarchical_clustering/#code","title":"code","text":"<ul> <li><code>scipy.cluster.hierarchy.linkage</code></li> <li><code>sklearn.cluster.AgglomerativeClustering</code></li> </ul>"},{"location":"other_models/hierarchical_clustering/#agglomerative-algorithm","title":"Agglomerative Algorithm:","text":"<ol> <li>Assign each record its own cluster (i.e. begin with smallest possible single-record clusters)</li> <li>Compute dissimilarity between all the pairs of clusters</li> <li>Merge two least dissimilar clusters</li> <li>Repeat 2-3 until some convergence criterion is met or all the records are merged into a single cluster.</li> </ol> <p>[[#Dissimilarity Measures|Dissimilarity]] is measured based on some distance metric. The distance is computed between all the pairs of dots in the two clusters.</p>"},{"location":"other_models/hierarchical_clustering/#dissimilarity-measures","title":"Dissimilarity Measures","text":""},{"location":"other_models/hierarchical_clustering/#complete-linkage","title":"Complete Linkage","text":"<p>Complete linkage is given by</p> <p>$$ \\text{Complete linkage} = \\max{d(a_i,b_j)}\\ i=\\overline{1,N}, j=\\overline{1,M} $$</p>"},{"location":"other_models/hierarchical_clustering/#single-linkage","title":"Single Linkage","text":"<p>Single linkage is given by</p> <p>$$ \\text{Single linkage} = \\min{d(a_i,b_j)}\\ i=\\overline{1,N}, j=\\overline{1,M} $$</p>"},{"location":"other_models/hierarchical_clustering/#average-linkage","title":"Average Linkage","text":"<p>Average linkage is given by</p> <p>$$ \\text{Average linkage} = \\frac{1}{n\\cdot m}\\sum_{i=1}^n{\\sum_{j=1}^m{d(a_i,b_j)}} $$</p>"},{"location":"other_models/hierarchical_clustering/#minimum-variance","title":"Minimum Variance","text":"<p>Another dissimilarity metric is called minimum variance (or Ward\u2019s method), which tends to minimize within-cluster sum of squares, like in K-Means.</p>"},{"location":"other_models/hierarchical_clustering/#dendrogram","title":"Dendrogram","text":"<p>Dendrogram is another way to get the appropriate number of clusters. It shows dissimilarity score between the merged clusters.</p> <p>![[assets/img/Machine Learning/Unsupervised Learning Models/Hierarchial Clustering/01.png|500]]</p>"},{"location":"other_models/hierarchical_clustering/#code-scipyclusterhierarchydendrogram","title":"code <code>scipy.cluster.hierarchy.dendrogram</code>.","text":""},{"location":"other_models/hierarchical_clustering/#sourcecode-hierarchial-clustering-codedendrogram","title":"sourcecode [[Hierarchial Clustering Code#Dendrogram]].","text":""},{"location":"other_models/k_means/","title":"K-Means Clustering","text":"<p>K-means was the first clustering method.</p> <p>The algorithm is straightforward:</p> <ol> <li>Start with K random cluster means</li> <li>Assign each point to the closest cluster</li> <li>Get new cluster means as the center of all points in the cluster</li> <li>Repeat 2-3 until convergence</li> </ol> <p>The exact solution is too computationally difficult, so this is a more efficient solution, but it is suggested to run multiple times on different subsets of data.</p> <p>Elbow chart is a way to get the appropriate number of clusters for the task. It shows number of clusters on x-axis versus explained variance on the y-axis.</p> <p>For K-means, it can be produced by accessing model\u2019s attribute <code>inertia_</code> and refitting the algorithm multiple times.</p>"},{"location":"other_models/knn/","title":"K-Nearest Neighbors","text":"<p>K-nearest neighbors (KNN) is a data-based approach for both classification and regression.</p> <p>The KNN algorithm is pretty easy:</p> <ul> <li>Find $K$ records that have all similar features as the new record.</li> <li>For classification, return the mode class of those records.</li> <li>For regression, return the mean target value of those records.</li> </ul> <p>The hyperparameter K is usually selected between 1 and 20.</p> <p>Similarity is measured using one of possible distance metrics, e.g. [[base/Statistics/Notation#Manhattan Distance|Manhattan]] or [[base/Statistics/Notation#Euclidean Distance|Euclidean]].</p>"},{"location":"other_models/knn/#code","title":"code","text":"<ul> <li><code>sklearn.neighbors.KNeighborsRegressor</code></li> <li><code>sklearn.neighbors.KNeighborsClassifier</code></li> </ul> <p>Inputs must be normalized with one of possible ways: -   subtract mean and divide by standard deviation (standard scaler) -   subtract minimum value and divide by range (<code>MinMaxScaler</code>) -   subtract median and divide by interquartile range (robust to outliers)</p> <p>The idea is to put all predictors on the same scale</p> <p>\ud83d\udca1 #warning Only Linear and Logistic Regression requires to drop one of the categories of one-hot encoded factor variables. Other techniques do not.</p>"},{"location":"other_models/naive_bayes/","title":"Naive Bayes","text":""},{"location":"other_models/naive_bayes/#code-sklearnnaive_bayesmultinomialnb","title":"code <code>sklearn.naive_bayes.MultinomialNB</code>.","text":""},{"location":"other_models/naive_bayes/#bayes-theorem","title":"Bayes\u2019 Theorem","text":"<p>The theorem is stated mathematically as follows</p> <p>$$ P(A|B)=\\frac{P(A)P(B|A)}{P(B)} $$</p> <p>where $P(A|B)$ is a conditional probability, i.e. probability of event $A$ given that event $B$ happened.</p>"},{"location":"other_models/naive_bayes/#naive-bayes-algorithm","title":"Naive Bayes Algorithm","text":"<p>Applying the Bayes\u2019 theorem to a dataset, the value of interest is $P(Y=i|X_1...X_p)$, i.e. the probability of $Y$ being either 0 or 1 ($i$) given the set of predictor variables, can be calculated as follows:</p> <p>$$ P(Y=i|X_1...X_p)=\\frac{P(Y=i)P(X_1...X_p|Y=i)}{P(Y=0)P(X_1...X_p|Y=0) + P(Y=1)P(X_1...X_p|Y=1)} $$</p> <p>\ud83d\udca1 Given even a relatively small number of predictors\u2014say, five\u2014it would require an overwhelmingly large dataset to find at least a couple of records with the same state of predictors $X_1...X_p$ to get conditional probabilities $P(X_1...X_p|Y=i)$.</p> <p>Under the assumption of conditional independence among predictor variables (naive assumption) all the mutual probabilities can be factorized:</p> <p>$$ P(X_1...X_p|Y=i)=\\prod_{j=1}^p{P(X_j|Y=i)} $$</p> <p>Changing the above equation into:</p> <p>$$ P(Y=i|X_1...X_p)=\\frac{P(Y=i)\\prod_{j=1}^p{P(X_j|Y=i)}}{P(Y=0)\\prod_{j=1}^p{P(X_j|Y=0)} + P(Y=1)\\prod_{j=1}^p{P(X_j|Y=1)}} $$</p>"},{"location":"other_models/naive_bayes/#supported-predictors","title":"Supported Predictors","text":"<p>The Bayesian classifier works only with categorical predictors. To apply naive Bayes to numerical predictors, one of two approaches must be taken:</p> <ul> <li>Bin and convert the numerical predictors to categorical predictors</li> <li>Use a probability model (e.g. normal distribution) for estimating the conditional probabilities $P(X_j|Y=i)$</li> </ul> <p>\ud83d\udca1 #warning Categorical variables for naive Bayes must be one-hot encoded without omitting.</p>"},{"location":"other_models/naive_bayes/#examples","title":"Examples","text":""},{"location":"other_models/naive_bayes/#bayes-theorem-example","title":"Bayes Theorem #example","text":"<p>There\u2019s a rare subspecies of a beetle, 0.1% of the total population. 98% of rare subspecies have a special pattern on the back, compared to only 5% members of the common subspecies. How likely is the beetle having the pattern to be rare?</p>"},{"location":"other_models/naive_bayes/#solution","title":"Solution","text":"<p>Given that:</p> <p>$$ P(\\text{Rare})=0.001 \\ P(\\text{Common})=0.999 \\ P(\\text{Pattern|Rare})=0.98 \\ P(\\text{Pattern|Common })=0.05 $$</p> <p>According to the Bayes\u2019 theorem:</p> <p>$$ P(\\text{Rare|Pattern}) = \\frac{P(\\text{Rare})P(\\text{Pattern|Rare})}{P(\\text{Pattern})} $$</p> <p>Expanding the $P(\\text{Pattern})$:</p> <p>$$ P(\\text{Rare|Pattern}) = \\frac{P(\\text{Rare})P(\\text{Pattern|Rare})}{P(\\text{Common})P(\\text{Common|Pattern}) + P(\\text{Rare})P(\\text{Rare|Pattern})} $$</p> <p>Substitute the values:</p> <p>$$ P(\\text{Rare|Pattern}) = \\frac{0.001 \\cdot 0.98}{0.999\\cdot 0.05 + 0.001\\cdot 0.98} = 0.019 $$</p> <p>That is, 1.9%.</p>"},{"location":"other_models/pca/","title":"Principal Component Analysis","text":"<p>\ud83d\udca1 #story Principal component analysis (PCA) was proposed by Karl Pearson and was presumably the first paper on unsupervised learning.</p> <p>The idea behind PCA is that variables often vary together, so PCA tends to reduce the number of variables by creating a smaller number of predictors (principal components) which model the most of the original probability</p> <p>New axes are linear combinations of previous and all orthogonal to each other.</p>"},{"location":"other_models/svm/","title":"Support Vector Machines","text":""},{"location":"other_models/svm/#classification","title":"Classification","text":"<p>Similarly to [[Linear Models#Logistic Regession|Logistic Regression]], the score of SVM classifier is a dot product of instance\u2019s feature vector and the model\u2019s weight vector plus intercept</p> <p>$$ \\text{Decision function}=\\boldsymbol{w}^T\\boldsymbol{x}+b $$</p> <p>Yet no function is applied to it, so there\u2019s no \u201cprobability\u201d score in SVM. The prediction is drawn straight from the decision function</p> <p>$$ \\hat{y}=  \\begin{cases}      0 &amp; \\text{if} &amp; \\boldsymbol{w}^T\\boldsymbol{x}+b&lt;0,     \\      1 &amp; \\text{if} &amp; \\boldsymbol{w}^T\\boldsymbol{x}+b\\ge0 \\end{cases} $$</p> <p>\ud83d\udca1Unlike most <code>sklearn</code> models, SVC doesn\u2019t have the <code>predict_proba</code> method since there\u2019s no probability sense assigned to the decision function.</p> <p>Under the hood SVM tries to get the widest possible margin between the two classes.</p> <p>Instances outside the margin do not affect the decision boundary while instances at the margin edge determine the boundary. Those are called support vectors.</p> <p>For data with only a single feature the train objective can be visualized as finding the the least possible slope so that the decision function is greater than $1$ for all positive instances and less than $-1$ for all negative instances:</p> <p>![[assets/img/Machine Learning/Support Vector Machines/01.png|600]]</p>"},{"location":"other_models/svm/#sourcecode-support-vector-machines-codewidest-margin-search","title":"sourcecode [[Support Vector Machines Code#Widest Margin Search]].","text":"<p>Considering whether instances are allowed to violate the $[-1;1]$ margin or not there\u2019s 2 main approaches:</p> <ul> <li>hard margin classification: instances are not allowed to violate the margin. Only applicable for linearly separable classes.</li> <li>soft margin classification: instances are allowed to violate the margin. Number of violations is minimized along with maximizing the margin width.</li> </ul> <p>Hard margin linear SVM classifier objective can be expressed as a constrained optimization problem</p> <p>$$ \\begin{aligned} \\underset{\\boldsymbol{w},b}{\\text{minimize}} \\quad &amp; \\frac{1}{2}|\\boldsymbol{w}|_2^2 \\ \\text{subject to} \\quad &amp; t^{(i)}(\\boldsymbol{w}^T\\boldsymbol{x}^{(i)}+b) \\ge 1,\\ i=\\overline{1,m} \\end{aligned} $$</p> <p>where</p> <p>$$ t^{(i)}= \\begin{cases} -1 &amp; \\text{if} &amp; y^{(i)}=0, \\ +1 &amp; \\text{if} &amp; y^{(i)}=1, \\ \\end{cases} $$</p> <p>Similarly, soft margin linear SVM classifier objective can be expressed as</p> <p>$$ \\begin{aligned} \\underset{\\boldsymbol{w},b,\\zeta}{\\text{minimize}} \\quad &amp; \\frac{1}{2}|\\boldsymbol{w}|2^2 + C\\sum^m{\\zeta^{(i)}}\\ \\text{subject to} \\quad &amp; t^{(i)}(\\boldsymbol{w}^T\\boldsymbol{x}^{(i)}+b) \\ge 1 - \\zeta^{(i)},\\ i=\\overline{1,m} \\end{aligned} $$</p> <p>where $\\zeta^{(i)}\\ge0$ is a slack variable, $C$ is a regularization hyperparameter (inverse proportional to the $\\ell_2$ penalty strength).</p> <p>SGD formulation can be expressed as minimizing the hinge cost function plus $\\ell_2$ penalty:</p> <p>$$ J(\\boldsymbol{w},b)=C\\sum_{i=1}^m\\max{\\left(0,1-t^{(i)}\\left(\\boldsymbol{w}^T\\boldsymbol{x}^{(i)}+b\\right)\\right)} + \\frac{1}{2}\\boldsymbol{w}^T\\boldsymbol{w} $$</p>"},{"location":"other_models/svm/#code","title":"code","text":"<ul> <li><code>sklearn.svm.LinearSVC</code> (fastest for linear tasks)</li> <li><code>sklearn.svm.SVC(kernel=\"linear\")</code> (supports kernel trick, slower on linear tasks than <code>LinearSVC</code>)</li> <li><code>sklearn.linear_model.SGDClassifier(loss=\"hinge\", alpha=1 / m / C)</code> (the slowest option, yet supports out-of-core training. Recommended for big datasets)</li> </ul>"},{"location":"other_models/svm/#regression","title":"Regression","text":"<p>Similarly to Linear Regression model, SVM regression equation is given by</p> <p>$$ \\hat{y}=\\boldsymbol{w}^T\\boldsymbol{x}+b $$</p> <p>SVM regressor tries to fit as many instances as possible within the specified $\\epsilon$ range. Training instances within the range do not affect the model.</p>"},{"location":"other_models/svm/#code_1","title":"code","text":"<ul> <li><code>sklearn.svm.LinearSVR</code> (fastest for linear tasks)</li> <li><code>sklearn.svm.SVR</code> (supports kernel trick, slower on linear tasks than <code>LinearSVR</code>)</li> <li><code>sklearn.linear_model.SGDRegressor(loss=\"hinge\", alpha=1 / m / C)</code> (supports out-of-core and online training, slowest among all)</li> </ul>"},{"location":"other_models/svm/#non-linear-models","title":"Non-Linear Models","text":""},{"location":"other_models/svm/#polynomial-features","title":"Polynomial Features","text":"<p>Adding higher degrees and permutations of the predictor variables is the same as for [[Linear Models#Polynomial Regression|polynomial regression]].</p>"},{"location":"other_models/svm/#gaussian-radial-basis-function","title":"Gaussian Radial Basis Function","text":"<p>Gaussian Radial Basis Function (RBF) is given by</p> <p>$$ \\phi_\\gamma(\\boldsymbol{x},\\ell)=\\exp{\\left(-\\gamma|\\boldsymbol{x}-\\ell|^2\\right)} $$</p> <p>where $\\ell$ is the landmark, $\\gamma$ (inverse proportional to bell width) is regularization parameter.</p> <p>You can think of landmark being every instance in the dataset. </p> <p>For #example take dummy data with one feature ($x$) with linearly non inseparable classes and replace it with two features ($x_1,\\ x_2$) of $\\phi_\\gamma(\\boldsymbol{x},4)\\ \\phi_\\gamma(\\boldsymbol{x},7)$, respectively. In the new space classes are linearly separable.</p> <p>![[assets/img/Machine Learning/Support Vector Machines/03.png|700]]</p>"},{"location":"other_models/svm/#sourcecode-support-vector-machines-codegaussian-radial-basis-function","title":"sourcecode [[Support Vector Machines Code#Gaussian Radial Basis Function]].","text":""},{"location":"other_models/svm/#quadratic-programming","title":"Quadratic Programming","text":"<p>The above stated optimization problems (i.e. convex quadratic optimization problems with linear constraints) belong to Quadratic Programming (QP) problems. Depending other conditions (i.e. kernel type) either the primal or the dual problem is solved.</p>"},{"location":"other_models/svm/#primal-problem","title":"Primal Problem","text":"<p>The primal problem is given by</p> <p>$$ \\begin{aligned} \\underset{\\boldsymbol{p}}{\\text{minimize}} \\quad &amp; \\frac{1}{2}\\boldsymbol{p}^T\\boldsymbol{H}\\boldsymbol{p} + \\boldsymbol{f} ^T\\boldsymbol{p}\\ \\text{subject to} \\quad &amp; \\boldsymbol{A}\\boldsymbol{p}\\le \\boldsymbol{b} \\end{aligned} $$</p>"},{"location":"other_models/svm/#dual-problem","title":"Dual Problem","text":"<p>The dual problem is given by</p> <p>$$ \\begin{aligned}</p> <p>\\underset{\\boldsymbol{\\alpha}}{\\text{minimize}} \\quad &amp; \\frac{1}{2}\\sum_{i=1}^m{\\sum_{j=1}^m{\\boldsymbol{\\alpha}^{(i)}\\boldsymbol{\\alpha}^{(j)}}\\boldsymbol{t}^{(i)}\\boldsymbol{t}^{(j)}\\boldsymbol{X}^{(i)^T}\\boldsymbol{X}^{(j)}}</p> <p>\\</p> <p>\\text{subject to} \\quad &amp; \\boldsymbol{\\alpha}^{(i)}\\ge 0,\\ i=\\overline{1,m}</p> <p>\\end{aligned} $$</p> <p>then the model\u2019s weight vector and bias can be expressed through the $\\boldsymbol{\\alpha}$ vector.</p> <p>\ud83d\udca1 $\\boldsymbol{\\alpha}^{(i)}\\ne0$ only for support vectors.</p>"},{"location":"other_models/svm/#kernelized-svm","title":"Kernelized SVM","text":"<p>Let\u2019s start with an example. Let $\\phi(\\boldsymbol{x})$ be a second-degree polynomial transformation. E.g. for a two-dimensional feature vector $\\boldsymbol{x}$</p> <p>$$ \\phi(\\boldsymbol{x})=\\phi\\left( \\begin{pmatrix} x_1 \\ x_2 \\end{pmatrix} \\right) = \\begin{pmatrix} x_1^2 \\ \\sqrt{2}x_1x_2\\ x_2^2 \\end{pmatrix} $$</p>  \ud83d\udca1 The exact formula of the $\\phi(\\boldsymbol{x})$ is of no importance here and can be safely omitted.   <p>Now let\u2019s calculate the dot product of two arbitrary 2-dimensional vectors $\\boldsymbol{a}, \\boldsymbol{b}$ transformed by $\\phi$</p> <p>$$ \\phi(\\boldsymbol{a})^T\\phi(\\boldsymbol{b})= \\begin{pmatrix} a_1^2 \\ \\sqrt{2}a_1a_2\\ a_2^2 \\end{pmatrix}^T \\begin{pmatrix} b_1^2 \\ \\sqrt{2}b_1b_2\\ b_2^2 \\end{pmatrix} = a_1^2b_1^2+2\\sqrt{2}a_1a_2b_1b_2+a_2^2b_2^2=\\left(a_1b_1+a_2b_2\\right)^2=\\left( \\begin{pmatrix} a_1 \\ a_2 \\end{pmatrix}^T \\begin{pmatrix} b_1 \\ b_2 \\end{pmatrix} \\right)^2 $$</p> <p>$$ \\boxed{ \\phi(\\boldsymbol{a})^T\\phi(\\boldsymbol{b})= \\left(\\boldsymbol{a}^T\\boldsymbol{b}\\right)^2} $$</p> <p>i.e. the dot product of transformed vectors equals to the square dot product of the original vectors, known as second-degree polynomial kernel.</p> <p>Similarly, other kernel functions are given by</p> <p>$$ \\begin{aligned} \\text{Linear:} \\quad &amp; K(\\boldsymbol{a},\\boldsymbol{b})=\\boldsymbol{a}^T\\boldsymbol{b} \\ \\text{Polynomial:} \\quad &amp; K(\\boldsymbol{a},\\boldsymbol{b})=\\left(\\gamma\\boldsymbol{a}^T\\boldsymbol{b} + r\\right)^d \\ \\text{Gaussian RBF:} \\quad &amp; K(\\boldsymbol{a},\\boldsymbol{b})=\\exp\\left(-\\gamma\\left|\\boldsymbol{a} - \\boldsymbol{b}\\right|^2\\right) \\ \\text{Sigmoid:} \\quad &amp; K(\\boldsymbol{a},\\boldsymbol{b})=\\tanh{\\left(\\gamma\\boldsymbol{a}^T\\boldsymbol{b} + r\\right)} \\end{aligned} $$</p> <p>Kernel is a function capable of computing the dot product $\\phi(\\boldsymbol{a})^T\\phi(\\boldsymbol{b})$ based only on the original vectors and not the transformation. This allows to just substitute the kernel expression into the [[#Dual Problem| dual problem]] equation</p> <p>$$ \\phi(\\boldsymbol{X}^{(i)})^T\\phi(\\boldsymbol{X}^{(j)}) \\rightarrow K(\\boldsymbol{X}^{(i)^T}\\boldsymbol{X}^{(j)}) $$</p> <p>without applying the transformation (e.g. in case of polynomial transformation, this won\u2019t cause the combinatorial explosion of the number of features; let alone Gaussian RBF function which maps the original feature vectors into infinite dimensional space).</p>"},{"location":"other_models/svm/#complexity","title":"complexity","text":"<ul> <li><code>LinearSVR</code>,<code>LinearSVC</code>: $O(m \\times n)$</li> <li><code>SVR</code>, <code>SVC</code>: $O(m^2 \\times n)$ to $O(m^3\\times n)$</li> <li><code>SGDRegressor</code>, <code>SGDClassifier</code>: $O(m\\times n)$</li> </ul>"},{"location":"other_models/svm/#examples","title":"Examples","text":""},{"location":"other_models/svm/#moons-classification-example","title":"Moons classification #example","text":"<p>Moons synthetic data looks like this</p> <p>![[assets/img/Machine Learning/Support Vector Machines/02.png|500]]</p> <p>Code (<code>LinearSVC</code> + <code>PolynomialFeatures</code>):</p> <pre><code>from sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nimport plotly.graph_objects as go\nimport numpy as np\n\nX, y = make_moons(n_samples=150, noise=0.15, random_state=42)\n\nmodel = Pipeline(\n    [\n        (\"features\", PolynomialFeatures(degree=3)),\n        (\"scale\", StandardScaler()),\n        (\"fit\", LinearSVC(C=10)),\n    ]\n)\nmodel.fit(X, y)\n\nx1 = np.linspace(X[:, 0].min(), X[:, 0].max())\nx2 = np.linspace(X[:, 1].min(), X[:, 1].max())\n\nX_test = np.meshgrid(x1, x2)\nX_test = np.c_[X_test[0].reshape((-1,)), X_test[1].reshape((-1,))]\n\ny_test = model.decision_function(X_test)\n\ngo.Figure(\n    data=(\n        go.Scatter(\n            x=X[:, 0],\n            y=X[:, 1],\n            mode=\"markers\",\n            marker=dict(color=y, symbol=y, colorscale=\"Tropic\"),\n        ),\n        go.Heatmap(\n            z=y_test.reshape(50, 50),\n            x=x1,\n            y=x2,\n            zsmooth=\"best\",\n            colorscale=\"Tropic\",\n            colorbar=dict(title_text=\"Decision function\"),\n        ),\n        go.Contour(\n            z=y_test.reshape(50, 50),\n            x=x1,\n            y=x2,\n            showscale=False,\n            colorscale=\"Greys\",\n            contours_coloring=\"lines\",\n            contours=dict(start=0, end=0),\n        ),\n    ),\n    layout=dict(\n        width=600,\n        height=400,\n        margin=dict(b=10, t=10, l=10, r=10),\n        plot_bgcolor=\"rgba(0,0,0,0)\",\n        xaxis=dict(visible=False),\n        yaxis=dict(visible=False),\n    ),\n).write_image(\"newplot.png\", scale=2)\n</code></pre> <p>Similarly, with <code>SVC</code> and kernel trick (ceteris paribus):</p> <pre><code>...\n\nmodel = Pipeline(\n    [\n        (\"scale\", StandardScaler()),\n        (\"fit\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5)),\n    ]\n)\n\n...\n</code></pre>"},{"location":"trees/decision_trees/","title":"Decision Trees","text":"<ul> <li><code>sklearn.tree.DecisionTreeClassifier</code></li> <li><code>sklearn.tree.DecisionTreeRegressor</code></li> </ul>"},{"location":"trees/decision_trees/#introduction","title":"Introduction","text":"<p>\ud83d\udca1 #story Tree models, trees, decision trees, or classification and regression trees (CART) were initially developed by Leo Breiman et al. in 1984.</p> <p>Trees are most commonly used in more powerful ensemble models: bagging ensembles (see [[base/Machine Learning/Random Forest|Random Forest]]) and boosted ensembles (see [[Gradient Boosted Trees]]) and almost never standalone.</p> <p>Tree is basically a set of if-else rules (branches) splitting data per one feature at a time to minimize data impurity in both splits (leaves). Trees can thus discover nonlinear patterns in data and spot important variables and [[Regression#Interactions|interactions]].</p> <p>Like regression models, a single decision tree is a [[base/Statistics/Notation#White Box Model|White Box]] model compared to tree-based ensembles which are [[base/Statistics/Notation#Black Box Model|Black Box]] models.</p>"},{"location":"trees/decision_trees/#recursive-partitioning","title":"Recursive Partitioning","text":"<p>Decision trees available in scikit-learn are CART trees. The training algorithm used in CART is called recursive partitioning:</p> <ol> <li>For each predictor variable $X_j$:<ol> <li>Split the dataset into two partitions for each value of $X_{i,j}$.</li> <li>Measure cost function for each pair of possible partitions.</li> <li>Record the value of $X_{i,j}$ of the best split (i.e. the lowest loss).</li> </ol> </li> <li>Select the best split among all best splits of each variable.</li> <li>Repeat 1-2 for each of the resulting splits.</li> </ol> <p>The cost function is based on one of [[#Impurity Measures]] and given by</p> <p>$$ J(k, t_k) = \\frac{m_1}{m}I_1 + \\frac{m_2}{m}I_2 $$</p> <p>for a selected threshold $t_k$ of a particular predictor variable $k$. Here $m_i/m$ are sample fractions and $I_i$ are [[#Impurity Measures|impurity scores]].</p> <p>\ud83d\udca1 #warning Recursive partitioning is produces only binary trees. Other algorithms (e.g. ID3, C4.5; Quinlan, 1993) can make more than 2 splits at a time.</p>"},{"location":"trees/decision_trees/#impurity-measures","title":"Impurity Measures","text":""},{"location":"trees/decision_trees/#gini-impurity","title":"Gini Impurity","text":"<p>Gini Impurity is given by</p> <p>$$ G_i=1 - \\sum_{k=1}^n{p_{i,k}} $$</p>"},{"location":"trees/decision_trees/#entropy","title":"Entropy","text":"<p>Entropy is given by</p> <p>$$ H_i=-\\sum_{k=1}^n{p_{i,k}\\log_2{p_{i,k}}} $$</p>"},{"location":"trees/decision_trees/#mean-squared-error","title":"Mean Squared Error","text":"<p>[[Regression Metrics#Mean Squared Error MSE|MSE]] is used as impurity measure for numeric target variables.</p>"},{"location":"trees/decision_trees/#regularization-hyperparameters","title":"Regularization Hyperparameters","text":"<p>The [[#Recursive Partitioning]] algorithm goes on until it meets one of the stop conditions listed below. These stop conditions act like regularization hyperparameters preventing the tree from overfitting -   Number of samples at leaf is less than <code>min_samples_leaf</code>. -   The fraction of sum of weights at leaf is less than <code>min_weight_fraction_leaf</code> (for weighted samples; otherwise just fraction of number of samples). -   Number of samples at branch is less than <code>min_samples_split</code>. -   Depth of the tree (i.e. the number of steps between the root and the farthest leaf) is more than <code>max_depth</code>. -   Number of leaves is more than <code>max_leaf_nodes</code>. -   Decrease of impurity of the split is less than <code>min_impurity_decrease</code>.</p>"},{"location":"trees/decision_trees/#classification","title":"Classification","text":"<p>Predicted class $\\hat{y}$ is simply the mode class of the leaf node.</p> <p>Class probability $\\hat{p}$ is calculated as the ratio of the number predicted class instances to all instances of the leaf node.</p>"},{"location":"trees/decision_trees/#regression","title":"Regression","text":"<p>Predicted value $\\hat{y}$ is simply the average value of all the instances of the leaf node.</p>"},{"location":"trees/decision_trees/#complexity","title":"#complexity","text":"<p>Making a prediction (i.e. traversing the tree): given that decision trees are generally balanced, is $O(\\log_2{m})$ and independent of the number of features.</p> <p>Training requires comparison among all features and all samples at each node $O(n\\times m\\log_2{m})$.</p>"},{"location":"trees/decision_trees/#examples","title":"Examples","text":""},{"location":"trees/decision_trees/#iris-classification-example","title":"Iris Classification #example","text":"<p>Based on a set of 150 records with four variables (petal length, petal width, sepal length, and sepal width), predict one of three types of iris\u2014Setosa, Virginica, or Versicolour.</p>"},{"location":"trees/decision_trees/#solution","title":"Solution","text":"<p>![[assets/img/Machine Learning/Decision Trees/01.png|500]]</p> <pre><code>from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nmodel = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nmodel.fit(X_train, y_train)\n\nfeature_names = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"]\nclass_names = [\"Setosa\", \"Versicolour\", \"Virginica\"]\n\nplot_tree(\n    model,\n    feature_names=feature_names,\n    class_names=class_names,\n    node_ids=True,\n)\nplt.show()\n</code></pre> <p>Text representation is also available:</p> <pre><code>&gt;&gt;&gt; from sklearn.tree import export_text\n&gt;&gt;&gt; export_text(model, feature_names=feature_names)\n|--- Petal Width &lt;= 0.80\n|   |--- class: 0\n|--- Petal Width &gt;  0.80\n|   |--- Petal Length &lt;= 4.95\n|   |   |--- class: 1\n|   |--- Petal Length &gt;  4.95\n|   |   |--- class: 2\n</code></pre>"},{"location":"trees/gradient_boosted_trees/","title":"Gradient Boosted Trees","text":"<p>The general idea of boosting is to combine a collection of weak learners so that each one corrects its predecessor.</p> <p>The most common boosting algorithms are [[#AdaBoost]], [[#Gradient Boosting]], and [[#Stochastic Gradient Boosting]].</p> <p>\ud83d\udca1 #warning An important drawback of boosting algorithms is that training cannot be parallelized: hence the linear increase of the training time with the number of predictors.</p>"},{"location":"trees/gradient_boosted_trees/#adaboost","title":"AdaBoost","text":"<p>\ud83d\udca1 #warning The algorithm below is given only for a classification task.</p> <p>Adaptive Boosting (or AdaBoost) starts with uniform sample weight distribution assigning each sample's weight  $\\boldsymbol{w}^{(i)}$ to $1 / m$.</p> <p>For each predictor the error rate is given by</p> <p>$$ r_j =     \\sum_{\\substack{i=1 \\ \\hat{y}\\ne y}}^m{\\boldsymbol{w}^{(i)}}      \\bigg/     \\sum_{i=1}^{m}{\\boldsymbol{w}^{(i)}} $$</p> <p>Based on the error rate each predictor is assigned its weight $\\alpha_j$</p> <p>$$ \\alpha_j = \\eta \\log{\\frac{1 - r_j}{r_j}} $$</p> <p>where $\\eta$ is learning rate hyperparameter ($\\eta = 1$ by default).</p> <p>Then instance weights are updated</p> <p>$$ \\boldsymbol{w}^{(i)} \\leftarrow \\begin{cases} \\boldsymbol{w}^{(i)} &amp; \\text{if}\\ \\hat{y}^{(i)}=y^{(i)} \\ \\boldsymbol{w}^{(i)} \\exp{\\alpha_j} &amp; \\text{if}\\ \\hat{y}^{(i)} \\ne y^{(i)} \\end{cases}</p> <p>\\quad i=\\overline{1,m} $$</p> <p>and renormalized (i.e. divided by $\\sum_{i=1}^m{\\boldsymbol{w}^{(i)}}$)</p> <p>The process is repeated until the last predictor is reached or until a perfect model is found.</p> <p>Ensemble predictions are then given by the majority of the weighted votes</p> <p>$$ \\hat{y} = \\underset{k}{\\text{argmin}} \\sum_{         \\substack{j=1 \\ \\hat{y}=k}     }^N{\\alpha_j} $$</p> <p>where $N$ is the number of predictors.</p>"},{"location":"trees/gradient_boosted_trees/#code","title":"code","text":"<ul> <li><code>sklearn.ensemble.AdaBoostClassifier</code></li> <li><code>sklearn.ensemble.AdaBoostRegressor</code></li> </ul>"},{"location":"trees/gradient_boosted_trees/#gradient-boosting","title":"Gradient Boosting","text":"<p>-</p>"},{"location":"trees/gradient_boosted_trees/#continue-from-page-267","title":"Continue from page 267","text":"<p>Gradient boosting instead casts the problem as an optimization of a cost function by fitting the model to a pseudo-residual, which has effect of training more heavily on larger residuals.</p> <p>Stochastic gradient boosting in addition applies random predictor and record sampling at each step.</p>"},{"location":"trees/gradient_boosted_trees/#stochastic-gradient-boosting","title":"Stochastic Gradient Boosting","text":"<p>The most widely used boosting implementation is <code>xgboost</code> (stochastic gradient boosting), which has <code>XGBClassifier</code> and <code>XGBRegressor</code> classes.</p>"},{"location":"trees/random_forest/","title":"Random Forest","text":"<p>Random Forest is an [[Ensemble Learning|ensemble]] of [[Decision Trees|Decision Trees]], generally trained via [[Ensemble Learning#Bagging|bagging]] (or sometimes [[Ensemble Learning#Pasting|pasting]]).</p> <p>Decision Trees in Random Forest are trained in the same way ([[Decision Trees#Recursive Partitioning|Recursive Partitioning]]) except for at each partition step only random subset of predictors (usually $\\sqrt{n}$) are considered.</p> <p>Random Forest has rougly all [[Decision Trees#Regularization Hyperparameters|Decision Tree]] hyperparameters plus all [[Ensemble Learning#Bagging|Bagging Ensemble]] hyperparameters.</p> <p>\ud83d\udca1 #warning Despite being built of a series of [[base/Statistics/Notation#White Box Model|White Box]] models random forest is a [[base/Statistics/Notation#Black Box Model|Black Box]] model.</p> <p>\ud83d\udca1 Random forest also provides variable importance scores, which can be accessed via <code>feature_importances_</code> property of a fitted model. It returns predictor-wise mean decrease in the [[Decision Trees#Gini Impurity|Gini Impurity]] score.</p>"},{"location":"trees/random_forest/#code","title":"code","text":"<ul> <li><code>sklearn.ensemble.RandomForestClassifier</code></li> <li><code>sklearn.ensemble.RandomForestRegressor</code></li> </ul>"},{"location":"trees/random_forest/#extra-trees","title":"Extra-Trees","text":"<p>Extra-Trees (or Extremely Randomized Trees) takes the [[#Random Forest]] training constraints one step further by assigning random thresholds during [[Decision Trees#Recursive Partitioning|Recursive Partitioning]] instead of searching for a threshold minimizing the [[Decision Trees#Impurity Measures|cost function]].</p> <p>This results in even higher [[base/Statistics/Notation#Bias|bias]] and lower [[base/Statistics/Notation#Variance|variance]].</p> <p>\ud83d\udca1The API of Extra-Trees models in scikit-learn is identical to [[base/Machine Learning/Random Forest]].</p>"},{"location":"trees/random_forest/#code_1","title":"code","text":"<ul> <li><code>sklearn.ensemble.ExtraTreesClassifier</code></li> <li><code>sklearn.ensemble.ExtraTreesRegressor</code></li> </ul>"},{"location":"trees/random_forest/#examples","title":"Examples","text":""},{"location":"trees/random_forest/#assessing-feature-importance-example","title":"Assessing Feature Importance #example","text":"<p>Feature importance of individual pixels of MNIST dataset</p> <p>![[assets/img/Machine Learning/Random Forest/01.png|500]]</p>"},{"location":"trees/random_forest/#sourcecode-random-forest-codefeature-importance","title":"sourcecode [[Random Forest Code#Feature Importance]].","text":""}]}